{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f333a0-7450-4136-b8cc-416e07426279",
   "metadata": {
    "id": "f6f333a0-7450-4136-b8cc-416e07426279"
   },
   "source": [
    "# Interroger des documents (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ca9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f9ca9bf",
    "outputId": "4112b46b-4271-4696-cf31-393e9e7ff8b3"
   },
   "outputs": [],
   "source": [
    "%pip install -qU wget openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af281185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a5a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client configuration\n",
    "base_url = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "session = requests.session()\n",
    "session.headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadba81-54dd-48ba-b6f0-fc8307e822c3",
   "metadata": {
    "id": "daadba81-54dd-48ba-b6f0-fc8307e822c3"
   },
   "source": [
    "Commençons par télécharger le document que nous souhaitons interroger. Ce document peut être un pdf, un fichier html ou un fichier json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80daa99-3416-4b81-a8aa-4fb7427bbe6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e80daa99-3416-4b81-a8aa-4fb7427bbe6c",
    "outputId": "abf16516-2ef9-40c3-dcad-74b6f9aa42e6"
   },
   "outputs": [],
   "source": [
    "# Download a file\n",
    "file_path = \"my_document.pdf\"\n",
    "if not os.path.exists(file_path):\n",
    "    doc_url = \"https://www.legifrance.gouv.fr/download/file/rxcTl0H4YnnzLkMLiP4x15qORfLSKk_h8QsSb2xnJ8Y=/JOE_TEXTE\"\n",
    "    wget.download(doc_url, out=file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RkAjTc20Agr9",
   "metadata": {
    "id": "RkAjTc20Agr9"
   },
   "source": [
    "Pour commencer, nous créons une collection nommée `tutorial`. Pour cela nous effectuons une requête GET sur l'endpoint `/v1/models` afin d'obtenir la liste des modèles disponibles et définissons le modèle d'embeddings à utiliser.\n",
    "\n",
    "Nous allons avoir besoin également d'un modèle de langage. Nous appelons le endpoint `/v1/models` pour obtenir la liste des modèles. Les modèles de langage ont le type *text-generation* et les modèles d'embeddings le type *text-embeddings-inference*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Q_5YNzmR_JcK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_5YNzmR_JcK",
    "outputId": "01554f0f-3d01-4946-993f-56e657904898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language model: AgentPublic/llama3-instruct-8b\n",
      "embeddings model: BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "language_model, embeddings_model = None, None\n",
    "\n",
    "for model in client.models.list().data:\n",
    "    if model.type == \"text-generation\" and language_model is None:\n",
    "        language_model = model.id\n",
    "    if model.type == \"text-embeddings-inference\" and embeddings_model is None:\n",
    "        embeddings_model = model.id\n",
    "\n",
    "print(f\"language model: {language_model}\\nembeddings model: {embeddings_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f0adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection ID: 6c6dd988-bd3d-4449-bfdd-106cda5d22ad\n"
     ]
    }
   ],
   "source": [
    "collection = \"tutorial\"\n",
    "\n",
    "response = session.post(f\"{base_url}/collections\", json={\"name\": collection, \"model\": embeddings_model})\n",
    "response = response.json()\n",
    "collection_id = response[\"id\"]\n",
    "print(f\"Collection ID: {collection_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9615d41-5ce2-471b-bd6c-90cfb2b78d21",
   "metadata": {
    "id": "a9615d41-5ce2-471b-bd6c-90cfb2b78d21"
   },
   "source": [
    "Enfin pour nous importons le document dans la collection de notre base vectorielle à l'aide du endpoint POST `/v1/files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6852fc7a-0b09-451b-bbc2-939fa96a4d28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6852fc7a-0b09-451b-bbc2-939fa96a4d28",
    "outputId": "8555033d-d20f-4b0b-8bfa-7fa5c83a299b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [201]>\n"
     ]
    }
   ],
   "source": [
    "files = {\"file\": (os.path.basename(file_path), open(file_path, \"rb\"), \"application/pdf\")}\n",
    "data = {\"request\": '{\"collection\": \"%s\"}' % collection_id}\n",
    "response = session.post(f\"{base_url}/files\", data=data, files=files)\n",
    "assert response.status_code == 201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ec73c-3e83-4266-a8de-c6a198f317b4",
   "metadata": {
    "id": "f78ec73c-3e83-4266-a8de-c6a198f317b4"
   },
   "source": [
    "Nous pouvons observer que le fichier que nous avons importé est bien dans la collection à l'aide du endpoint GET `/v1/documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6d6140-5c91-4c3e-9350-b6c8550ab145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd6d6140-5c91-4c3e-9350-b6c8550ab145",
    "outputId": "0ddea4bb-889e-4ebc-a912-7a2e461ea987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in collection: 2\n"
     ]
    }
   ],
   "source": [
    "response = session.get(f\"{base_url}/documents/{collection_id}\")\n",
    "assert response.status_code == 200\n",
    "files = response.json()[\"data\"]\n",
    "print(f\"Number of files in collection: {len(files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3aed3",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre collection et notre fichier, nous pouvons faire une recherche vectorielle à l'aide du endpoint POST `/v1/search`. Ces résutats de recherche vectorielle seront utilisés pour générer une réponse à l'aide du modèle de langage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244b5de",
   "metadata": {},
   "source": [
    "## Les méthodes de recherche\n",
    "\n",
    "Trois méthodes de recherche sont disponibles :\n",
    "- lexicale\n",
    "- sémantique (méthode par défault)\n",
    "- hybride \n",
    "\n",
    "### Lexicale\n",
    "\n",
    "La méthode lexicale est la plus simple. Elle ne fait pas de recherche vectorielle mais se base uniquement sur la similarité lexicale entre la question et le contenu des documents à l'aide de l'algorithme [BM25](https://en.wikipedia.org/wiki/Okapi_BM25).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d071a7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents, Ulrich Tan est le chef du pôle Datamin du département \"Étalab\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt, \"method\": \"lexical\"}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "rag_prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789180a5",
   "metadata": {},
   "source": [
    "## Sémantique (méthode par défaut)\n",
    "\n",
    "La méthode sémantique se base sur la similarité vectorielle (similarité cosinus) entre la question et la représentation vectorielle des documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt, \"method\": \"semantic\"}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data)\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dac78d",
   "metadata": {},
   "source": [
    "## Hybride\n",
    "\n",
    "La méthode hybride est une combinaison de la méthode lexicale et de la méthode vectorielle. Elle se base sur la similarité lexicale entre la question et le contenu des documents mais également sur la similarité vectorielle entre la question et le contenu des documents. Pour plus d'informations voir [cet article](https://weaviate.io/blog/hybrid-search-explained).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a27806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents fournis, Ulrich Tan est chef du pôle Datamin du département \"Étalab\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt, \"method\": \"hybrid\"}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b100b95",
   "metadata": {},
   "source": [
    "## Recherche sur internet\n",
    "\n",
    "Vous pouvez également faire ajouter une recherche sur internet en spécifiant \"internet\" dans la liste des collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f374c1ad-b5ec-4870-a11a-953c7d219f94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f374c1ad-b5ec-4870-a11a-953c7d219f94",
    "outputId": "64279978-1ae5-4bac-f028-bb0899d83d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents, Ulrich Tan est le chef du DataLab au sein de la Direction interministérielle du numérique (Dinum), où il est responsable de coordonner l'équipe du DataLab et d'accompagner les acteurs publics dans l'identification et la priorisation de cas d'usage d'intelligence artificielle pour leur administration. Il est également considéré comme un \"jeune quadra génie du numérique\" et a été embauché par l'État un an avant pour introduire l'intelligence artificiale à différents étages de l'administration pour la rendre plus efficace et plus rapide, à la fois pour les fonctionnaires et les citoyens.\n"
     ]
    }
   ],
   "source": [
    "data = {\"collections\": [\"internet\"], \"k\": 6, \"prompt\": prompt}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data)\n",
    "\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "rag_prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a0492",
   "metadata": {},
   "source": [
    "On peut observer que les sources sont des pages web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f982989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.lefigaro.fr/conjoncture/ulrich-tan-cet-ingenieur-qui-introduit-l-ia-dans-les-administrations-pour-les-rendre-plus-efficaces-20240422\n",
      "https://www.etalab.gouv.fr/datalab/equipe/\n"
     ]
    }
   ],
   "source": [
    "for source in sources:\n",
    "    print(source)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
