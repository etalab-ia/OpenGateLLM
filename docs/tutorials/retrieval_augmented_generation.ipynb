{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f333a0-7450-4136-b8cc-416e07426279",
   "metadata": {
    "id": "f6f333a0-7450-4136-b8cc-416e07426279"
   },
   "source": [
    "# Interroger des documents (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ca9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f9ca9bf",
    "outputId": "4112b46b-4271-4696-cf31-393e9e7ff8b3"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.11.9 nécessitent le package ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Créer un environnement Python</a> avec les packages requis.\n",
      "\u001b[1;31mOu installez « ipykernel » en utilisant la commande : « /usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall »"
     ]
    }
   ],
   "source": [
    "%pip install -qU wget openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af281185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a5a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client configuration\n",
    "base_url = \"http://localhost:8501/v1\"\n",
    "api_key = \"sk-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo0LCJ0b2tlbl9pZCI6NSwiZXhwaXJlc19hdCI6MTc1MDQ1NjgwMH0.48k-xFMIVJlzYdkgVMalojR7_JTyxpvV6hNv5AIPv7Y\"\n",
    "\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "session = requests.session()\n",
    "session.headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadba81-54dd-48ba-b6f0-fc8307e822c3",
   "metadata": {
    "id": "daadba81-54dd-48ba-b6f0-fc8307e822c3"
   },
   "source": [
    "Commençons par télécharger le document que nous souhaitons interroger. Ce document peut être un pdf, un fichier html ou un fichier json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80daa99-3416-4b81-a8aa-4fb7427bbe6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e80daa99-3416-4b81-a8aa-4fb7427bbe6c",
    "outputId": "abf16516-2ef9-40c3-dcad-74b6f9aa42e6"
   },
   "outputs": [],
   "source": [
    "# Download a file\n",
    "file_path = \"/Users/acor/marker_v2/albert-api/docs/tutorials/IA107.pdf\"\n",
    "if not os.path.exists(file_path):\n",
    "    doc_url = \"https://www.legifrance.gouv.fr/download/file/rxcTl0H4YnnzLkMLiP4x15qORfLSKk_h8QsSb2xnJ8Y=/JOE_TEXTE\"\n",
    "    wget.download(doc_url, out=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RkAjTc20Agr9",
   "metadata": {
    "id": "RkAjTc20Agr9"
   },
   "source": [
    "Pour commencer, nous créons une collection nommée `tutorial`. Pour cela nous effectuons une requête GET sur l'endpoint `/v1/models` afin d'obtenir la liste des modèles disponibles et définissons le modèle d'embeddings à utiliser.\n",
    "\n",
    "Nous allons avoir besoin également d'un modèle de langage. Nous appelons le endpoint `/v1/models` pour obtenir la liste des modèles. Les modèles de langage ont le type *text-generation* et les modèles d'embeddings le type *text-embeddings-inference*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Q_5YNzmR_JcK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_5YNzmR_JcK",
    "outputId": "01554f0f-3d01-4946-993f-56e657904898"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_set_private_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m language_model, embeddings_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m language_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         language_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mid\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/resources/models.py:91\u001b[0m, in \u001b[0;36mModels.list\u001b[0;34m(self, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPage[Model]:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Lists the currently available models, and provides basic information about each\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    one such as the owner and availability.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSyncPage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1291\u001b[0m, in \u001b[0;36mSyncAPIClient.get_api_list\u001b[0;34m(self, path, model, page, body, options, method)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_api_list\u001b[39m(\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1282\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1289\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPageT:\n\u001b[1;32m   1290\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m-> 1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_api_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1142\u001b[0m, in \u001b[0;36mSyncAPIClient._request_api_list\u001b[0;34m(self, model, page, options)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[1;32m   1140\u001b[0m options\u001b[38;5;241m.\u001b[39mpost_parser \u001b[38;5;241m=\u001b[39m _parser\n\u001b[0;32m-> 1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1124\u001b[0m, in \u001b[0;36mSyncAPIClient._process_response\u001b[0;34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(RAW_RESPONSE_HEADER)):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[0;32m-> 1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_response.py:325\u001b[0m, in \u001b[0;36mAPIResponse.parse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    323\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(to\u001b[38;5;241m=\u001b[39mto)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mpost_parser):\n\u001b[0;32m--> 325\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[1;32m    328\u001b[0m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_id)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1133\u001b[0m, in \u001b[0;36mSyncAPIClient._request_api_list.<locals>._parser\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parser\u001b[39m(resp: SyncPageT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPageT:\n\u001b[0;32m-> 1133\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_private_attributes\u001b[49m(\n\u001b[1;32m   1134\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1136\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1137\u001b[0m     )\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_set_private_attributes'"
     ]
    }
   ],
   "source": [
    "language_model, embeddings_model = None, None\n",
    "\n",
    "for model in client.models.list().data:\n",
    "    if model.type == \"text-generation\" and language_model is None:\n",
    "        language_model = model.id\n",
    "    if model.type == \"text-embeddings-inference\" and embeddings_model is None:\n",
    "        embeddings_model = model.id\n",
    "\n",
    "print(f\"language model: {language_model}\\nembeddings model: {embeddings_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f0adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection ID: 39a36180-aab5-4a5a-8e07-4da01a8b8193\n"
     ]
    }
   ],
   "source": [
    "collection = \"tutorial\"\n",
    "\n",
    "response = session.post(f\"{base_url}/collections\", json={\"name\": collection, \"model\": embeddings_model})\n",
    "response = response.json()\n",
    "collection_id = response[\"id\"]\n",
    "print(f\"Collection ID: {collection_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9615d41-5ce2-471b-bd6c-90cfb2b78d21",
   "metadata": {
    "id": "a9615d41-5ce2-471b-bd6c-90cfb2b78d21"
   },
   "source": [
    "Enfin pour nous importons le document dans la collection de notre base vectorielle à l'aide du endpoint POST `/v1/files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6852fc7a-0b09-451b-bbc2-939fa96a4d28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6852fc7a-0b09-451b-bbc2-939fa96a4d28",
    "outputId": "8555033d-d20f-4b0b-8bfa-7fa5c83a299b"
   },
   "outputs": [],
   "source": [
    "files = {\"file\": (os.path.basename(file_path), open(file_path, \"rb\"), \"application/pdf\")}\n",
    "data = {\"request\": '{\"collection\": \"%s\"}' % collection_id}\n",
    "response = session.post(f\"{base_url}/files\", data=data, files=files)\n",
    "assert response.status_code == 201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ec73c-3e83-4266-a8de-c6a198f317b4",
   "metadata": {
    "id": "f78ec73c-3e83-4266-a8de-c6a198f317b4"
   },
   "source": [
    "Nous pouvons observer que le fichier que nous avons importé est bien dans la collection à l'aide du endpoint GET `/v1/documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6d6140-5c91-4c3e-9350-b6c8550ab145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd6d6140-5c91-4c3e-9350-b6c8550ab145",
    "outputId": "0ddea4bb-889e-4ebc-a912-7a2e461ea987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in collection: 2\n"
     ]
    }
   ],
   "source": [
    "response = session.get(f\"{base_url}/documents/{collection_id}\")\n",
    "assert response.status_code == 200\n",
    "files = response.json()[\"data\"]\n",
    "print(f\"Number of files in collection: {len(files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3aed3",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre collection et notre fichier, nous pouvons faire une recherche vectorielle à l'aide du endpoint POST `/v1/search`. Ces résutats de recherche vectorielle seront utilisés pour générer une réponse à l'aide du modèle de langage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244b5de",
   "metadata": {},
   "source": [
    "## Les méthodes de recherche\n",
    "\n",
    "Trois méthodes de recherche sont disponibles :\n",
    "- lexicale\n",
    "- sémantique (méthode par défault)\n",
    "- hybride \n",
    "\n",
    "### Lexicale\n",
    "\n",
    "La méthode lexicale est la plus simple. Elle ne fait pas de recherche vectorielle mais se base uniquement sur la similarité lexicale entre la question et le contenu des documents à l'aide de l'algorithme [BM25](https://en.wikipedia.org/wiki/Okapi_BM25).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2668210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d071a7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents, Ulrich Tan est le chef du pôle Datamin du département \"Étalab\".\n"
     ]
    }
   ],
   "source": [
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt, \"method\": \"lexical\"}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "rag_prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789180a5",
   "metadata": {},
   "source": [
    "## Sémantique (méthode par défaut)\n",
    "\n",
    "La méthode sémantique se base sur la similarité vectorielle (similarité cosinus) entre la question et la représentation vectorielle des documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt, \"method\": \"semantic\"}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data)\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dac78d",
   "metadata": {},
   "source": [
    "## Hybride\n",
    "\n",
    "La méthode hybride est une combinaison de la méthode lexicale et de la méthode vectorielle. Elle se base sur la similarité lexicale entre la question et le contenu des documents mais également sur la similarité vectorielle entre la question et le contenu des documents. Pour plus d'informations voir [cet article](https://weaviate.io/blog/hybrid-search-explained).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a27806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents fournis, Ulrich Tan est chef du pôle Datamin du département \"Étalab\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt, \"method\": \"hybrid\"}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b100b95",
   "metadata": {},
   "source": [
    "## Recherche sur internet\n",
    "\n",
    "Vous pouvez également faire ajouter une recherche sur internet en spécifiant \"internet\" dans la liste des collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f374c1ad-b5ec-4870-a11a-953c7d219f94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f374c1ad-b5ec-4870-a11a-953c7d219f94",
    "outputId": "64279978-1ae5-4bac-f028-bb0899d83d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents, Ulrich Tan est le chef du DataLab au sein de la Direction interministérielle du numérique (Dinum), où il est responsable de coordonner l'équipe du DataLab et d'accompagner les acteurs publics dans l'identification et la priorisation de cas d'usage d'intelligence artificielle pour leur administration. Il est également considéré comme un \"jeune quadra génie du numérique\" et a été embauché par l'État un an avant pour introduire l'intelligence artificiale à différents étages de l'administration pour la rendre plus efficace et plus rapide, à la fois pour les fonctionnaires et les citoyens.\n"
     ]
    }
   ],
   "source": [
    "data = {\"collections\": [\"internet\"], \"k\": 6, \"prompt\": prompt}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data)\n",
    "\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "rag_prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": rag_prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a0492",
   "metadata": {},
   "source": [
    "On peut observer que les sources sont des pages web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f982989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.lefigaro.fr/conjoncture/ulrich-tan-cet-ingenieur-qui-introduit-l-ia-dans-les-administrations-pour-les-rendre-plus-efficaces-20240422\n",
      "https://www.etalab.gouv.fr/datalab/equipe/\n"
     ]
    }
   ],
   "source": [
    "for source in sources:\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00610e",
   "metadata": {},
   "source": [
    "## Recherche par `/chat/completions`\n",
    "\n",
    "Il est possible de faire une recherche RAG avec l'endpoint `/chat/completions`. Pour ce faire, il faut spécifier le paramètre `search=True` et `search_args` avec les arguments de la recherche que vous souhaitez faire (idem que pour l'endpoint `/search`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2e1368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Réponse du model: Selon les documents, Ulrich Tan est le chef du pôle Datamin du département \"Eatalab\".\n",
      "\n",
      "- Sources utilisées pour la génération:\n",
      "\n",
      "–  M. Gilles Corbi, agent contractuel, chef du pôle Production du département « ISO » ; \n",
      "–  M. Yann Brûlé, grade, chef du pôle « SI RIE » du département « ISO » ; \n",
      "–  M. Ulrich Tan, chef du pôle Datamin du département « Etalab » ; \n",
      "–  Mme Marie-Christie Ritz, cheffe du pôle « RH et attractivité » ; \n",
      "–  Mme Fadila Leturcq, cheffe du pôle « Campus du numérique ». \n",
      "\n",
      "\f21  juin 2023 \n",
      "\n",
      "JOURNAL  OFFICIEL  DE  LA  RÉPUBLIQUE  FRANÇAISE \n",
      "\n",
      "Texte 5 sur 95\n",
      "–  Mme Elsa Le Duigou, agent contractuel ; \n",
      "–  M. Valentin Brel, agent contractuel ; \n",
      "–  Mme Margot Sanchez, agent contractuel ; \n",
      "–  M. Paul Burgun, agent contractuel ; \n",
      "–  Mme Emma Ghariani, agent contractuel. \n",
      "Art.  5.  –  Délégation est donnée à Mme Géraldine Taurand, attachée d’administration hors classe, cheffe de la \n",
      "mission « Budget et achats », pour signer tous actes de gestion et procéder aux validations électronique dans l’outil\n",
      "–  M. Perica Sucevic, agent contractuel, préfigurateur de la mission « Droit et international » ; \n",
      "–  Mme Floriane Beaudron, agente contractuelle, cheffe de la mission « Communication ». \n",
      "Art.  3.  –  Délégation est donnée à l’effet de signer au nom du ministre de la transformation et de la fonction \n",
      "publiques ainsi qu’au nom de la Première ministre, tous actes relevant des attributions de leurs pôles, et notamment\n",
      "–  M. Ishan Bhojwani, agent contractuel, chef du département « Incubateur de services numériques » (ISN) ; \n",
      "–  Mme Marielle Chrisment, lieutenante-colonelle de gendarmerie, cheffe du département « Etalab » ; \n",
      "–  M.  Guy Duplaquet,  ingénieur général  des mines,  chef du  département « Infrastructures et services  opérés » \n",
      "\n",
      "(ISO) ; \n",
      "\n",
      "–  M.  Florian  Delezenne,  agent  contractuel,  chef  du  département  « Opérateur  de  produits  interministériels » \n",
      "\n",
      "(OPI).\n",
      "–  M.  Louis  di  Benedetto,  inspecteur  en  chef  de  la  santé  publique  vétérinaire,  chef  du  pôle  « Pilotage  de  la \n",
      "\n",
      "qualité et des partenariats » du département « ISO » ; \n",
      "\n",
      "–  M.  Philippe  Levillain,  agent  contractuel,  chef  du  pôle  « Réseau  interministériel  de  l’Etat  /  SOI »  du \n",
      "\n",
      "département « ISO » ;\n",
      "–  Mme Virginie Rozière, ingénieure en chef de l’armement, cheffe du pole « Expertise, contrôle et maitrise des \n",
      "\n",
      "risques » du département « ACE » ; \n",
      "\n",
      "–  Mme Marine Boudeau, agente contractuelle, préfiguratrice du pôle « Brigade d’intervention numérique », du \n",
      "\n",
      "département « ACE » ; \n",
      "\n",
      "–  M. Mohsen Souissi, agent contractuel, préfigurateur du pôle « Référentiels, capitalisation et observatoires » du \n",
      "\n",
      "département « ACE » ;\n"
     ]
    }
   ],
   "source": [
    "response = session.post(\n",
    "    url=f\"{base_url}/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"model\": language_model,\n",
    "        \"stream\": False,\n",
    "        \"n\": 1,\n",
    "        \"search\": True,\n",
    "        \"search_args\": {\"collections\": [collection_id], \"k\": 6, \"method\": \"semantic\"},\n",
    "    },\n",
    ")\n",
    "response = response.json()\n",
    "\n",
    "sources = [result[\"chunk\"][\"content\"] for result in response[\"search_results\"]]\n",
    "\n",
    "print(f\"\"\"- Réponse du model: {response['choices'][0]['message']['content']}\n",
    "\n",
    "- Sources utilisées pour la génération:\n",
    "\n",
    "{'\\n'.join(sources)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a2396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
