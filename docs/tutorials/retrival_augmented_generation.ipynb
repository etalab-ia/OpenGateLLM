{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f333a0-7450-4136-b8cc-416e07426279",
   "metadata": {
    "id": "f6f333a0-7450-4136-b8cc-416e07426279"
   },
   "source": [
    "# Interroger des documents (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ca9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f9ca9bf",
    "outputId": "4112b46b-4271-4696-cf31-393e9e7ff8b3"
   },
   "outputs": [],
   "source": [
    "%pip install -qU wget openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a5a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client configuration\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "base_url = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "session = requests.session()\n",
    "session.headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadba81-54dd-48ba-b6f0-fc8307e822c3",
   "metadata": {
    "id": "daadba81-54dd-48ba-b6f0-fc8307e822c3"
   },
   "source": [
    "Commençons par télécharger le document que nous souhaitons interroger. Ce document peut être un pdf, un fichier html ou un fichier json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80daa99-3416-4b81-a8aa-4fb7427bbe6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e80daa99-3416-4b81-a8aa-4fb7427bbe6c",
    "outputId": "abf16516-2ef9-40c3-dcad-74b6f9aa42e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_document (1).pdf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a file\n",
    "import wget\n",
    "import os\n",
    "\n",
    "file_path = \"my_document.pdf\"\n",
    "if not os.path.exists(file_path):\n",
    "    doc_url = \"https://www.legifrance.gouv.fr/download/file/rxcTl0H4YnnzLkMLiP4x15qORfLSKk_h8QsSb2xnJ8Y=/JOE_TEXTE\"\n",
    "\n",
    "wget.download(doc_url, out=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RkAjTc20Agr9",
   "metadata": {
    "id": "RkAjTc20Agr9"
   },
   "source": [
    "Pour commencer, nous créons une collection nommée `tutorial`. Pour cela nous effectuons une requête GET sur l'endpoint `/v1/models` afin d'obtenir la liste des modèles disponibles et définissons le modèle d'embeddings à utiliser.\n",
    "\n",
    "Nous allons avoir besoin également d'un modèle de langage. Nous appelons le endpoint `/v1/models` pour obtenir la liste des modèles. Les modèles de langage ont le type *text-generation* et les modèles d'embeddings le type *text-embeddings-inference*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Q_5YNzmR_JcK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_5YNzmR_JcK",
    "outputId": "01554f0f-3d01-4946-993f-56e657904898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language model: AgentPublic/llama3-instruct-8b\n",
      "embeddings model: BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "language_model, embeddings_model = None, None\n",
    "\n",
    "for model in client.models.list().data:\n",
    "    if model.type == \"text-generation\" and language_model is None:\n",
    "        language_model = model.id\n",
    "    if model.type == \"text-embeddings-inference\" and embeddings_model is None:\n",
    "        embeddings_model = model.id\n",
    "\n",
    "print(f\"language model: {language_model}\\nembeddings model: {embeddings_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f0adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection ID: a8ce92d1-4b38-48b2-8459-7ebe9ae84c2f\n"
     ]
    }
   ],
   "source": [
    "collection = \"tutorial\"\n",
    "\n",
    "response = session.post(f\"{base_url}/collections\", json={\"name\": collection, \"model\": embeddings_model})\n",
    "response = response.json()\n",
    "collection_id = response[\"id\"]\n",
    "print(f\"Collection ID: {collection_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9615d41-5ce2-471b-bd6c-90cfb2b78d21",
   "metadata": {
    "id": "a9615d41-5ce2-471b-bd6c-90cfb2b78d21"
   },
   "source": [
    "Enfin pour nous importons le document dans la collection de notre base vectorielle à l'aide du endpoint POST `/v1/files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6852fc7a-0b09-451b-bbc2-939fa96a4d28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6852fc7a-0b09-451b-bbc2-939fa96a4d28",
    "outputId": "8555033d-d20f-4b0b-8bfa-7fa5c83a299b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '4ab9f9d1-fbf7-40a4-81f0-892bab407f1f'}\n"
     ]
    }
   ],
   "source": [
    "files = {\"file\": (os.path.basename(file_path), open(file_path, \"rb\"), \"application/pdf\")}\n",
    "data = {\"request\": '{\"collection\": \"%s\"}' % collection_id}\n",
    "response = session.post(f\"{base_url}/files\", data=data, files=files)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ec73c-3e83-4266-a8de-c6a198f317b4",
   "metadata": {
    "id": "f78ec73c-3e83-4266-a8de-c6a198f317b4"
   },
   "source": [
    "Nous pouvons observer que le fichier que nous avons importé est bien dans la collection à l'aide du endpoint GET `/v1/files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd6d6140-5c91-4c3e-9350-b6c8550ab145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd6d6140-5c91-4c3e-9350-b6c8550ab145",
    "outputId": "0ddea4bb-889e-4ebc-a912-7a2e461ea987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "response = session.get(f\"{base_url}/files/{collection_id}\")\n",
    "response.json()\n",
    "files = response.json()[\"data\"]\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3aed3",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre collection et notre fichier, nous pouvons faire une recherche vectorielle à l'aide du endpoint POST `/v1/search`. Ces résutats de recherche vectorielle seront utilisés pour générer une réponse à l'aide du modèle de langage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ce73cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selon les documents, M. Ulrich Tan est le chef du pôle Datamin du département \"Eatalab\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [collection_id], \"k\": 6, \"prompt\": prompt}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11557a2",
   "metadata": {},
   "source": [
    "Nous avons récupéré les sources depuis les metadata de chaque chunk. Nous pouvons constater que les chunks proviennent bien du document que nous avons importé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35eda4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_document.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a122cb8",
   "metadata": {},
   "source": [
    "Vous pouvez également faire ajouter une recherche sur internet en spécifiant \"internet\" dans la liste des collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f374c1ad-b5ec-4870-a11a-953c7d219f94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f374c1ad-b5ec-4870-a11a-953c7d219f94",
    "outputId": "64279978-1ae5-4bac-f028-bb0899d83d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ulrich Tan est le responsable de coordonner l'équipe du DataLab et d'accompagner les acteurs publics dans l'identification et la priorisation de cas d'usage d'intelligence artificielle pour leur administration.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Qui est Ulrich Tan ?\"\n",
    "data = {\"collections\": [\"internet\"], \"k\": 6, \"prompt\": prompt}\n",
    "response = session.post(url=f\"{base_url}/search\", json=data, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
    "\n",
    "prompt_template = \"Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\\n\\nDocuments :\\n\\n{chunks}\"\n",
    "chunks = \"\\n\\n\\n\".join([result[\"chunk\"][\"content\"] for result in response.json()[\"data\"]])\n",
    "sources = set([result[\"chunk\"][\"metadata\"][\"document_name\"] for result in response.json()[\"data\"]])\n",
    "prompt = prompt_template.format(prompt=prompt, chunks=chunks)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    model=language_model,\n",
    "    stream=False,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f984e",
   "metadata": {},
   "source": [
    "Nous pouvons de la même manière récupérer les sources depuis les metadata de chaque chunk et les afficher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f982989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://www.etalab.gouv.fr/datalab/equipe/'}\n"
     ]
    }
   ],
   "source": [
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad96d70",
   "metadata": {},
   "source": [
    "Enfin il est possible de combiner une recherche dans les fichiers et une recherche sur internet."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
