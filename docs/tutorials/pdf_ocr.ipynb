{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2761a7ad-9a18-4455-8d5b-93891815a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Obtaining dependency information for pdf2image from https://files.pythonhosted.org/packages/62/33/61766ae033518957f877ab246f87ca30a85b778ebaad65b7f74fa7e52988/pdf2image-1.17.0-py3-none-any.whl.metadata\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pillow (from pdf2image)\n",
      "  Obtaining dependency information for pillow from https://files.pythonhosted.org/packages/b9/d8/f6004d98579a2596c098d1e30d10b248798cceff82d2b77aa914875bfea1/pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: pillow, pdf2image\n",
      "Successfully installed pdf2image-1.17.0 pillow-11.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a54993b-2e1d-4997-9db7-c060b91a305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=200):\n",
    "    \"\"\"\n",
    "    Convert a multi-page PDF into a list of PIL images (one per page).\n",
    "    Returns the list of images.\n",
    "    \"\"\"\n",
    "    return convert_from_path(pdf_path, dpi=dpi)\n",
    "\n",
    "def image_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert a PIL image to a base64-encoded PNG bytes string.\n",
    "    \"\"\"\n",
    "    import io\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)\n",
    "    img_str = base64.b64encode(buffer.read()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "def call_albert_api_with_image(base64_image, api_key, question=\"Please perform OCR on this page\"):\n",
    "    \"\"\"\n",
    "    Sends one image (base64) to the Albert API, along with a question prompt.\n",
    "    Returns the response JSON.\n",
    "    \"\"\"\n",
    "    url = \"https://albert.api.etalab.gouv.fr/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    model = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": question\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.15\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcecd2b4-17f6-4699-9644-7645e05e343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Page 1 OCR result:\n",
      "blank page\n",
      "\n",
      "Processing page 2...\n",
      "Page 2 OCR result:\n",
      "blank page\n",
      "\n",
      "Processing page 3...\n",
      "Page 3 OCR result:\n",
      "# Information\n",
      "\n",
      "**Warning**\n",
      "\n",
      "This document, written by ANSSI, the French National Cybersecurity Agency, is titled “Security recommendations for a generative AI system”. It is freely available at cyber.gouv.fr/en.\n",
      "\n",
      "It is an original creation from ANSSI and it is placed under the “Open Licence v2.0” published by the Etalab mission.\n",
      "\n",
      "According to the Open Licence v2.0, this document can be freely reused, subject to mentioning its paternity (source and date of last update). Reuse means the right to communicate, distribute, redistribute, publish, transmit, reproduce, copy, adapt, modify, extract, transform and use, including for commercial purposes\n",
      "\n",
      "The recommendations are provided as is and are related to threats known at the publication time. Considering the information systems diversity, ANSSI cannot guarantee direct application of these recommendations on targeted information systems. Applying the following recommendations shall be, at first, validated by IT administrators and/or IT security managers.\n",
      "\n",
      "This document is a courtesy translation of the initial French document “Recommandations de sécurité pour un système d’IA générative [29]”, available at cyber.gouv.fr. In case of conflicts between these two documents, the latter is considered as the only reference.\n",
      "\n",
      "**Document changelog:**\n",
      "\n",
      "| VERSION | DATE       | CHANGELOG     |\n",
      "|---------|------------|---------------|\n",
      "| 1.0     | 29/04/2024 | First version |\n",
      "\n",
      "SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM - 1\n",
      "\n",
      "Processing page 4...\n",
      "Page 4 OCR result:\n",
      "Contents\n",
      "\n",
      "1 Context 3\n",
      "1.1 Introduction .......................................................................................................... 3\n",
      "1.2 Definitions .......................................................................................................... 4\n",
      "1.3 Scope ................................................................................................................... 5\n",
      "\n",
      "2 Summary 6\n",
      "\n",
      "3 Description of a generative AI system 7\n",
      "3.1 Lifecycle of a generative AI system ................................................................ 7\n",
      "3.2 Architecture of a generative AI system .......................................................... 10\n",
      "\n",
      "4 Attack scenarios on generative AI 11\n",
      "\n",
      "5 Recommendations 14\n",
      "5.1 General recommendations .............................................................................. 14\n",
      "5.2 Recommendations for the training phase .................................................... 20\n",
      "5.3 Recommendations for the deployment phase ............................................. 21\n",
      "5.4 Recommendations for the production phase ............................................... 22\n",
      "5.5 Special case of AI-assisted source code generation ..................................... 25\n",
      "5.6 Special case of consumer AI services hosted on the Internet .................... 26\n",
      "5.7 Special case of using third-party generative AI solutions .......................... 26\n",
      "\n",
      "Recommendation List 29\n",
      "\n",
      "Bibliography 30\n",
      "\n",
      "Processing page 5...\n",
      "Page 5 OCR result:\n",
      "# 1 Context\n",
      "\n",
      "## 1.1 Introduction\n",
      "\n",
      "Artificial intelligence (AI) has long been a subject of research, but the potential offered by compute resources and big data has opened up new opportunities. These include a significant increase in the number of products that can generate a response to a question in natural language using a model trained on very large volume of data. These AI models are generally referred to as Large Language Models (LLMs) and fall into the category of generative AI (see definitions in section 1.2).\n",
      "\n",
      "The recent enthusiasm for these products and services, some of which have been made easily accessible to the public, has prompted organisations (businesses, government departments) to look at the potential productivity benefit that could be achieved using AI.\n",
      "\n",
      "While this technology offers new opportunities for the organisation of work, it is important to be vigilant and cautious in the approach to the deployment and integration of AI in an existing information system. The deployment of generative AI tools gives rise to new threats that could have a significant impact, for example on the confidentiality of the data processed by these tools, but also on the integrity of the information systems to which they are connected.\n",
      "\n",
      "The purpose of this document is to provide security recommendations for the use of generative AI solutions based on LLMs within public and private entities.\n",
      "\n",
      "Processing page 6...\n",
      "Page 6 OCR result:\n",
      "# 1.2 Definitions\n",
      "\n",
      "**Generative AI**\n",
      "\n",
      "Generative AI is a subsection of artificial intelligence, focused on creating models trained to generate content (text, images, videos, etc.) from a specific corpus of training data.\n",
      "\n",
      "**Large Language Model**\n",
      "\n",
      "A category of generative AI models that can generate text close to natural human language and which are generally trained on a large dataset.\n",
      "\n",
      "**AI Model**\n",
      "\n",
      "In the context of this guide, an AI model refers to a neural network and its parameters (weights, bias 1 ).\n",
      "\n",
      "**AI System**\n",
      "\n",
      "An AI system encompasses all the technical components of an application based on an AI model: implementation of this AI model, front-end services for users, databases, logging, etc.\n",
      "\n",
      "**Query**\n",
      "\n",
      "A query (or prompt) refers to the instruction in text form sent by the user to the AI system.\n",
      "\n",
      "**Adversarial Attack**\n",
      "\n",
      "An adversarial attack aims to send to an AI system one or more malicious requests with the aim of misleading or altering its proper operation.\n",
      "\n",
      "1. In a neural network, a weight is a power coefficient of the connection between 2 neurons, which is adjusted throughout the training phase. A bias is a constant linked to a neuron that allows for “compensation” in the calculation of the result.\n",
      "\n",
      "4 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 7...\n",
      "Page 7 OCR result:\n",
      "# 1.3 Scope\n",
      "\n",
      "This document deals mainly with the following use cases:\n",
      "\n",
      "- Producing a summary of documentation;\n",
      "- Retrieval of information or generating text from a corpus of documents;\n",
      "- Chatbot²;\n",
      "- Source code generation for software developers.\n",
      "\n",
      "The identified documentary corpus could be multi-modal, i.e. it may involve multiple categories of input data: text, image, sound, video, etc. However, the guide focuses primarily on the generation of output text and does not deal specifically with image or video generation (although most of the recommendations also apply to these use cases).\n",
      "\n",
      "This documentary corpus includes the model’s training data, but can also include additional data or documents supplied directly as input by the user.\n",
      "\n",
      "This document only deals with securing the architecture of a generative AI system based on an LLM.\n",
      "\n",
      "Security issues related to data quality³ and performance⁴ of an AI model are not covered in this document.\n",
      "\n",
      "Similarly, if other issues such as ethics, privacy, intellectual property, the protection of business secrecy, or the protection of personal data are also issues to be taken into account when designing an AI model, these do not fall within ANSSI’s area of expertise and are therefore not covered in this guide.\n",
      "\n",
      "For all of these subjects, you can consult the work of the following organisations: ENISA [6, 7], BSI [1], NIST [15, 16] or CNIL [2].\n",
      "\n",
      "ANSSI also co-signed an NCSC-UK [14] paper on AI security in November 2023.\n",
      "\n",
      "---\n",
      "\n",
      "2. A chatbot is defined here as an application enabling a written exchange between the user and the AI system rather than an oral exchange.\n",
      "\n",
      "3. Data quality generally refers to a business criterion. Data quality criteria from a business point of view could be, for example its origin, quantity, completeness, relevance, accuracy, representativeness (in a statistical sense), or conformity to a given structure.\n",
      "\n",
      "4. The performance of an AI model is also a business concept that is highly dependent on the objectives set when the model was designed. It can include a number of factors such as the accuracy, relevance or speed of responses generated for users, for example.\n",
      "\n",
      "Processing page 8...\n",
      "Page 8 OCR result:\n",
      "# 2 Summary\n",
      "\n",
      "The implementation of a generative AI system can be divided into 3 cyclical phases: an initial training phase for the AI model based on specifically selected data, then an integration and deployment phase, and finally an operational production phase in which users can access the trained AI model via the AI system.\n",
      "\n",
      "Each of these 3 phases must be covered by specific security measures, which depend in part on the outsourcing decision for each component (hosting, model training, performance testing, etc.) as well as the sensitivity of the data used in each phase and the criticality of the AI system regarding its business purpose.\n",
      "\n",
      "In addition to the traditional threats inherent in any information system, an AI system may be subject to specific attacks, for example disrupting its proper functioning (adversarial attacks) or to exfiltrate data processed by itself.\n",
      "\n",
      "The issue of data security, particularly training data, is therefore a key challenge for a generative AI system, linked with the information accessed by users when they query the model. Actually, it is designed to generate an answer from all the data accessed during training, as well as additional data that may come from sensitive internal sources.\n",
      "\n",
      "The use of a generative AI system must therefore meet confidentiality requirements (sending sensitive data to public tools available on the Internet[^5] must be unauthorized.) but also meet integrity and availability requirements. AI system interactions with other applications or IS components must therefore be secured, limited to strictly operational needs, and human validation must be implemented when it’s critical to the organisation.\n",
      "\n",
      "Specific uses cases, such as AI-assisted application development, raise a number of major issues and must therefore be managed (with great vigilance over sensitive modules or applications), checked by humans and tested regularly (with automatic source code analysis tools).\n",
      "\n",
      "Finally, protecting AI models themselves may be as much of an issue as data security not only for reasons of protecting scientific and technical capabilities (academic research, models used for defense and national security, etc.), but also because an attacker with knowledge of the architecture and parameters of an AI model may be able to improve its attack capabilities for other purposes (data exfiltration, etc.).\n",
      "\n",
      "[^5]: Such as ChatGPT, Gemini or even DeepL for translation.\n",
      "\n",
      "6 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 9...\n",
      "Page 9 OCR result:\n",
      "# Description of a generative AI system\n",
      "\n",
      "**Warning**\n",
      "\n",
      "The lifecycle and architecture presented in this chapter are given as examples to make the recommendations easier to understand. They are therefore not intended to be prescriptive. In particular, the sequence of the functions presented here is only one possible option. Depending on the use case, these functions may not always be implemented in an AI system.\n",
      "\n",
      "## 3.1 Lifecycle of a generative AI system\n",
      "\n",
      "Figure 1 describes an example of the lifecycle of a generative AI system.\n",
      "\n",
      "![Lifecycle of a generative AI system](image_url)\n",
      "\n",
      "**Figure 1 – Example of the lifecycle of a generative AI system**\n",
      "\n",
      "**AI Training**\n",
      "\n",
      "- Model Development\n",
      "- Data Cleaning\n",
      "- Pre-Training\n",
      "- Fine Tuning\n",
      "- Alignment\n",
      "- Performance Testing\n",
      "\n",
      "**AI Deployment**\n",
      "\n",
      "- Security Tests\n",
      "- Security Audit\n",
      "- Build and Packaging\n",
      "- Integration on IS\n",
      "- Release in Production\n",
      "\n",
      "**AI Production**\n",
      "\n",
      "- Maintenance / Patching\n",
      "- Supervision / Logs\n",
      "- Web Portal\n",
      "- Plugins\n",
      "- AI Model\n",
      "- Cache\n",
      "- Vector Database\n",
      "- Filters\n",
      "- Output\n",
      "\n",
      "*Model Re-Training*\n",
      "\n",
      "*Additional Data (internal or external)*\n",
      "\n",
      "*Orchestrator*\n",
      "\n",
      "Processing page 10...\n",
      "Page 10 OCR result:\n",
      "The 3 phases of training, deployment and production 6 may involve different technical environments and different users. It is important that security is treated in each of these 3 phases of a generative AI system’s lifecycle.\n",
      "\n",
      "These 3 phases can be carried out in different environments, for example the training phase in a public Cloud and the deployment and production phases locally within the entity. Nevertheless, appropriate security measures must be applied regardless of the environment chosen.\n",
      "\n",
      "Re-training an AI model structured in this way does not generally involve repeating all the steps in the training phase (very often, only the fine-tuning or alignment stages are carried out).\n",
      "\n",
      "Figure 2 shows examples of responsibility sharing throughout the design phases of a generative AI system.\n",
      "\n",
      "![Figure 2](image_url)\n",
      "\n",
      "**Figure 2 – Shared responsibility scenarios for a generative AI system**\n",
      "\n",
      "The security risks and impacts should be evaluated according to the scenario chosen by the organisation.\n",
      "\n",
      "6. The production phase may sometimes be called inference phase of the AI model, i.e. the model performs predictions for given users.\n",
      "\n",
      "8 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 11...\n",
      "Page 11 OCR result:\n",
      "Figure 3 describes the integration of a generative AI system into an IS and the important points regarding the internal and external interactions.\n",
      "\n",
      "![Figure 3](image_url)\n",
      "\n",
      "Figure 3 – Integration of a generative AI system into an existing IS\n",
      "\n",
      "Particular attention must be paid to these interactions and these must be included in the scope of the analysis in all phases of the project.\n",
      "\n",
      "**R1 Integrate security into all phases of the lifecycle of an AI system**\n",
      "\n",
      "Security measures must be identified and applied in each of the 3 phases of the AI system’s lifecycle: training, deployment and production. These measures depend heavily on the responsibility-sharing framework adopted and the associated subcontracting. They must also take into account interactions with other applications or components both within and external to the IS. Refer to the ANSSI cybersecurity hygiene guide [17] to ensure the basic level of security which is to be applied.\n",
      "\n",
      "SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM – 9\n",
      "\n",
      "Processing page 12...\n",
      "Page 12 OCR result:\n",
      "# 3.2 Architecture of a generative AI system\n",
      "\n",
      "Figure 4 describes an example of the general architecture of a generative AI system.\n",
      "\n",
      "![General architecture of a generative AI system](image_url)\n",
      "\n",
      "**Figure 4 – General architecture of a generative AI system**\n",
      "\n",
      "This architecture is not an exhaustive list of all the components of a generative AI system, but is designed to identify potential attack paths used to target an entity.\n",
      "\n",
      "There are a number of important elements in this schema:\n",
      "\n",
      "- the different populations with access to an AI system in each phase: users, developers, administrators, auditors, etc.;\n",
      "- the vector database, which is generally used to store additional data index in the form of vectors, in order to enrich[^7] user queries before sending them to the model (concept known as RAG - Retrieval Augmentation Generation). This database can be built from internal data sources of the organisation or external from partner sources;\n",
      "- the input and output filters of the AI model, providing defence-in-depth against malicious requests or undesired behaviour by the AI system;\n",
      "- the plugins or additional components, which can be used to connect the AI system to other business or technical resources within or outside the organisation.\n",
      "\n",
      "[^7]: A vector database is used in particular in the context of LLMs, because it can be used to establish comparisons and identify relationships between objects, and therefore understand better the context.\n",
      "\n",
      "10 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 13...\n",
      "Page 13 OCR result:\n",
      "# 4 Attack scenarios on generative AI\n",
      "\n",
      "A generative AI system is at first a standard business application, which must have the same security baseline as any other business application within the entity. However, in addition to this security baseline, the entity must take into account specific threats to a generative AI system.\n",
      "\n",
      "These threats can be divided into 3 main categories of attacks8:\n",
      "\n",
      "- **Manipulation attacks:** these attacks involve hijacking the behaviour of an AI system in production by malicious queries. These can lead to unexpected responses, dangerous actions or a denial of service;\n",
      "\n",
      "- **Infection attacks:** these attacks involve infecting an AI system during its training phase, by altering the training data or inserting a backdoor;\n",
      "\n",
      "- **Exfiltration attacks:** these attacks involve stealing information about an AI system in production, such as the data used to train the model, user data, or internal model data (parameters).\n",
      "\n",
      "In the context of generative AI, these attacks can affect the following security requirements:\n",
      "\n",
      "- **Confidentiality:** the aim is to protect an AI system against the sensitive data leaks: training datasets, user queries, model parameters, additional internal data, etc.;\n",
      "\n",
      "- **Integrity:** the objective is to protect an AI system against an unexpected change in its behaviour. Integrity can concern the model itself (parameters) or target the training datasets (poisoning) or even the technical components that enable the AI system to work properly: scripts9, external libraries (supply-chain attack), services configurations, etc.;\n",
      "\n",
      "- **Availability:** the objective is to protect an AI system against denial of service or actions intended to degrade its performance (malicious requests);\n",
      "\n",
      "- **Traceability:** the objective is to guarantee explicability10 and the accountability of actions carried out on an AI system. These elements can facilitate the investigation and remediation work following a security incident.\n",
      "\n",
      "8. These categories are from the CNIL taxonomy on this subject: [https://lnc.cnil.fr/petite-taxonomie-des-attaques-des-systemes-dia](https://lnc.cnil.fr/petite-taxonomie-des-attaques-des-systemes-dia).\n",
      "\n",
      "9. These scripts could be, for example, scripts for fine-tuning the AI model or scripts for the deployment or maintenance of an AI system.\n",
      "\n",
      "10. As defined by CNIL, explicability is the ability to make links between the objects and identify the criteria taken into account by the AI system in order to produce a result.\n",
      "\n",
      "Processing page 14...\n",
      "Page 14 OCR result:\n",
      "Figure 5 describes some examples of attacks on a generative AI system in an IS.\n",
      "\n",
      "![Figure 5](image_url)\n",
      "\n",
      "Figure 5 – Attack scenarios on a generative AI system in an IS\n",
      "\n",
      "1. the attacker has access to a data source and first poisons the data used to train the AI model, which means it can be then used for other purposes once the AI system is in production (e.g. trigger a malicious action based on a specific query);\n",
      "\n",
      "2. the attacker gains access to the development environment and inserts a backdoor into the code of the AI system (e.g. by directly altering the parameters of the model or the configuration of a technical component of the AI system);\n",
      "\n",
      "3. the attacker gains access to an external pre-deployment test service and hijacks the integration process (e.g. by sending a misleading or malicious result to the integration chain);\n",
      "\n",
      "4. either the attacker uses an adversarial attack technique to exfiltrate sensitive data processed by the AI model (e.g. retrieving training data or queries from other users of the service), or uses malicious queries to cause a denial of service;\n",
      "\n",
      "5. the attacker has access to an external resource accessed by the AI system and sends a malicious response which is integrated by the model (e.g. sending a URL which points to a malicious website controlled by the attacker);\n",
      "\n",
      "6. the attacker gains access to a plugin used by the AI system and injects malicious commands when performing an action on a business application (e.g. inserting malicious code into the body of an email generated by the AI system).\n",
      "\n",
      "In the context of this guide (LLM generative AI), the following effects can be identified:\n",
      "\n",
      "12 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 15...\n",
      "Page 15 OCR result:\n",
      "■ tarnishing the reputation of services available to the public by altering the proper functioning of generative AI systems (e.g. Chatbot);\n",
      "\n",
      "■ exfiltration of sensitive data from generative AI systems;\n",
      "\n",
      "■ theft of proprietary AI model parameters (weights)11;\n",
      "\n",
      "■ lateralisation of an attack to other business applications interconnected to generative AI systems (e.g. internal mailing system);\n",
      "\n",
      "■ sabotage of business applications by injecting vulnerabilities into AI-generated source code.\n",
      "\n",
      "A data leak of an entity’s sensitive data is a threat which must be taken into account for all cases, regardless of the use case of the generative AI system. The AI system must incorporate the issue of the access rights and respect information user access restrictions into the responses it provides.\n",
      "\n",
      "Particular attention should also be paid to indirect attack scenarios involving an AI system, such as the automatic generation of content for another application (insertion of malicious URLs in the response).\n",
      "\n",
      "Finally, the risk analysis must take into account the responsibility-sharing structure adopted for the project (see figure 2). For example, using a model trained by a third party may give rise to the risk of a supply-chain attack. Untrusted third-party providers can train the model to react in an unexpected manner when it is provided with a particular query. One of the risk reduction measures could be to audit the model itself or not to use it for critical applications.\n",
      "\n",
      "A risk analysis, carried out using the EBIOS-RM [23] method for example, should therefore start as early as possible, i.e. before the training phase.\n",
      "\n",
      "R2 Conduct a risk analysis on AI systems before the training phase\n",
      "\n",
      "Risk analysis of an AI system must address the following issues:\n",
      "\n",
      "■ Map all the elements linked to the AI model: third-party libraries, data sources, interconnected applications, etc.;\n",
      "\n",
      "■ Identify the sub-parts of the AI system that will process the organisation’s data, particularly the ones contained in user queries;\n",
      "\n",
      "■ Consider how responsibilities would be shared and how subcontracting would work for each of the phases;\n",
      "\n",
      "■ Identify the direct and indirect effects that incorrect or malicious responses from the AI model to users would have;\n",
      "\n",
      "■ Consider the security of AI model training data.\n",
      "\n",
      "The recommendations in the next chapter are designed to address these specific threats.\n",
      "\n",
      "11. Knowledge of model weights can also allow attackers to improve the capability for other attacks.\n",
      "\n",
      "Processing page 16...\n",
      "Page 16 OCR result:\n",
      "# 5 Recommendations\n",
      "\n",
      "## 5.1 General recommendations\n",
      "\n",
      "The use of external libraries and modules must be considered during the design phase of the project, in order to identify potential vulnerabilities associated with these modules. The goal is to provide the maximum level of protection against a supply-chain-attack targeting components required for the proper functioning of the AI system. Please refer to the ANSSI guide on digital risks [22] or the CISA documentation on this subject [8].\n",
      "\n",
      "### R3 Evaluate the level of confidence in the libraries and external modules used in the AI system\n",
      "\n",
      "It is recommended to map all libraries and external modules used in the project and to assess the level of confidence in these.\n",
      "\n",
      "In the same way as for software components (libraries and external modules), it is also essential to assess the sources of data not managed by the organisation. These sources could be training data sets retrieved from the Internet, model performance validation sets or additional data sets used during the production phase.\n",
      "\n",
      "### R4 Evaluate the level of confidence in external data sources used in the AI system\n",
      "\n",
      "It is recommended to map all external data sources used in the project and to assess the level of confidence in these.12\n",
      "\n",
      "In general, it is recommended to apply good development practices during the design and implementation of the AI system. These good practices are sometimes grouped together under the name DevSecOps or the term security-by-design. For further information, please refer to the ANSSI guide [28] on this subject, or to the NIST documentation on this subject [10], or follow the recommendations of the NCSC-UK [5] and the CISA [11].\n",
      "\n",
      "### R5 Apply DevSecOps principles to all phases of the project\n",
      "\n",
      "Secure development best practices should be applied throughout all phases of the project, for example:\n",
      "\n",
      "- Deploy and secure continuous integration and continuous deployment (CI/CD)\n",
      "\n",
      "12. The CNIL criteria (https://www.cnil.fr/fr/tenir-compte-de-la-protection-des-donnees-dans-la-collecte-et-la-gestion-des-donnees) or the proposal by Datasheets for Datasets (https://arxiv.org/pdf/1803.09010.pdf) can be used to evaluate an external dataset.\n",
      "\n",
      "Processing page 17...\n",
      "Page 17 OCR result:\n",
      "chains by applying the least privilege principle for access to the tools in these CI/CD chains;\n",
      "\n",
      "- Securely manage secrets used in all phases of the project;\n",
      "\n",
      "- Carry out automated security tests on the source code (static code analysis) and when the source code is executed (dynamic code analysis);\n",
      "\n",
      "- Protect the integrity of the source code and secure access to it (multi-factor authentication, code signature, access rights, etc.);\n",
      "\n",
      "- Use secure development languages (fine tuning scripts, model development, maintenance, deployment, etc.).\n",
      "\n",
      "In order to implement an AI model, the various parameters of this model (weight, bias, etc.) need to be stored in files. Several formats can be used for this purpose, some of which may present a risk of arbitrary code execution, such as those implementing functions for loading serialised objects. It is therefore preferable to use formats that strictly separate the model’s data parameters and the model’s executable code data.\n",
      "\n",
      "R6 Use secure AI model formats\n",
      "\n",
      "We recommend using state-of-the-art security formats, such as safetensor. Certain insecure formats, such as pickle, should be forbidden.\n",
      "\n",
      "Generative AI models will have to handle data throughout their lifecycle. Applying confidentiality protection measures to this data can be complicated for the following reasons:\n",
      "\n",
      "- The data may come from multiple sources and are sometimes combined together in the same set: public data, partner data, internal data, private data, etc.;\n",
      "\n",
      "- The volume of data can be very large, particularly in the case of LLM training, which makes processing more difficult;\n",
      "\n",
      "- Regular updates may be required, particularly when re-training the model;\n",
      "\n",
      "- Data may need to be pre-processed to lower the level of confidentiality (anonymisation, deletion of fields, etc.);\n",
      "\n",
      "- Data may be used across different phases of the project: during the model training phase, but also in production when the model needs to access additional data;\n",
      "\n",
      "- The data can include the AI system’s user data during the production phase: data from user queries and the responses provided by the AI model to these users.\n",
      "\n",
      "It is important to understand that an AI model inherits the sensitivity of the data that contributed to its training and also of the data used to re-train it. An AI model can be vulnerable to a phenomenon known as “regurgitation”. In some cases, for example, it may generate responses which are close to the training data, revealing potentially sensitive information.\n",
      "\n",
      "Depending on the responsibility-sharing scenario adopted (see chapter 2), the data confidentiality issues will be different, and the technical measures to protect against data exfiltration will need to be adapted. For example, if the entity wishes to subcontract the training phase to a service\n",
      "\n",
      "Processing page 18...\n",
      "Page 18 OCR result:\n",
      "provider, it is important to ensure that the data stored and processed by this service provider is kept sufficiently confidential (state-of-the-art encryption, isolation of resources from other customers, secure keys used, secure deletion after reallocation of resources, etc.).\n",
      "\n",
      "Similarly, a proprietary AI model that you want to keep confidential must be subject to specific security measures if it is stored in an untrusted environment (e.g. in a Cloud provider or embedded in physical hosted equipment, for example in the IoT).\n",
      "\n",
      "### Manage data confidentiality issues from the AI system design phase\n",
      "\n",
      "The project design must map all the data sets used in each phase of the AI system: training (training data sets), deployment (test sets) and production (additional data, vector database, etc.).\n",
      "\n",
      "This study must include users data from the AI system in production, i.e. user queries and the responses provided by the AI model.\n",
      "\n",
      "The analysis can also cover confidentiality protection for the parameters of the model itself, for proprietary model, for example.\n",
      "\n",
      "Access to an AI system also complicates the application of the users rights on the information. There are several categories of data to differentiate here:\n",
      "\n",
      "- **Training data:** it is not possible to manage user access rights on this data due to the structure of the neural networks;\n",
      "\n",
      "- **Additional data in production:** it is possible to manage access rights but this depends on the options offered by the tools used (RBAC 13) to store the information (local data management system, vector database, etc.);\n",
      "\n",
      "- **User data:** user queries and responses may contain sensitive data. These data are temporarily stored during processing in the AI system and sometimes are used to re-train the model (e.g. alignment with RLHF - Reinforcement learning from human feedback).\n",
      "\n",
      "The information access rights issue must therefore be considered again each time the model is retrained, including data derived from the use of the model in production (additional business data, user queries, etc.).\n",
      "\n",
      "### Manage users data access rights issue from the AI system design phase\n",
      "\n",
      "It is important to define the options for the model’s structure prior to the project in order to manage the need to know:\n",
      "\n",
      "- The choice of data used for training (without the ability to manage access rights) and additional data for production (with the ability to manage roles and access rights);\n",
      "\n",
      "- The model’s training strategy, i.e. when is the model re-trained and what underlying data is used for the model’s re-training (additional business data, user queries, model responses, etc.).\n",
      "\n",
      "13. Role Based Access Control\n",
      "\n",
      "Processing page 19...\n",
      "Page 19 OCR result:\n",
      "The majority of LLM generative AI have non-deterministic behaviour, and they can also be subject to hallucinations. This uncertainty around the response to a given query means that greater vigilance is required with regard to the indirect consequences of these responses. For example, the interaction of an AI system with other IS resources must not allow critical automated actions to the organisation to be executed.\n",
      "\n",
      "This precautionary principle must be applied and a generative AI system must not be able to make critical decisions that have a major impact on the business or the protection of assets and people without human intervention (e.g. validation in an HMI). In these particular cases, human capacity for discernment helps to reduce the risk of scenarios that could present a danger to the organisation.\n",
      "\n",
      "For example, it is important not to use an AI system to automate critical administrative actions on the entity’s technical infrastructure (e.g. discovery and automatic deployment of network configurations or firewall rules).\n",
      "\n",
      "Do not allow AI systems to run automatically critical actions on the IS\n",
      "\n",
      "R9\n",
      "\n",
      "An AI system must be configured so that it cannot automatically execute critical IS actions. These actions may be critical from a business point of view (banking transactions, production of public content, physical impact on humans, etc.) or critical actions on the IS infrastructure (reconfiguration of network components, creation of privileged users, deployment of virtual machines, etc.).\n",
      "\n",
      "The roles and access rights of AI system developers and administrators must be strictly defined and applied during the project. The principles of secure administration, as described in the ANSSI guide on this subject [21], must be applied in all phases of the AI system’s lifecycle.\n",
      "\n",
      "Manage and secure developer and administrator privileged access to the AI system\n",
      "\n",
      "R10\n",
      "\n",
      "All privileged operations on the AI system must comply with secure administration best practices, in particular:\n",
      "\n",
      "- Privileged operations must be defined and triggering these must be approved: re-training, modification of data sets, new interconnection with an application, change of hosting, etc.;\n",
      "- Privileged operations must be carried out using dedicated accounts and from a dedicated administration workstation;\n",
      "- The principle of least privilege must be applied and temporary authentication tokens should be used;\n",
      "- The development environment must be managed to the same level of security as the production environment.\n",
      "\n",
      "14. Phenomenon in which a model generates erroneous content that is not based on real data.\n",
      "\n",
      "Processing page 20...\n",
      "Page 20 OCR result:\n",
      "The hosting of the AI system, regardless of the phase it is in, must be considered. The level of security must be consistent with the project’s security requirements, and especially the confidentiality requirements for the data used in each phase.\n",
      "\n",
      "In particular, this point must be strictly applied for the model training phase because there are major threats during this phase, as we saw earlier in the chapter 4.\n",
      "\n",
      "Warning\n",
      "\n",
      "AI models are considered to have the same level of sensitivity as the data used to design and train them. Rule R9 [12] from the circular “Cloud au Centre” [13] must be applied in the case of French public administration.\n",
      "\n",
      "R11 Host the AI system in trusted environments consistent with security needs\n",
      "\n",
      "The hosting of the AI system during the 3 phases of the lifecycle must be consistent with the project’s security requirements, especially its confidentiality and integrity requirements. In particular, the security of the model’s training data (at rest, in transit, during processing) must not be overlooked.\n",
      "\n",
      "An AI system’s training, deployment and production environments must be siloed separately. This measure reduces the risk of lateralisation between environments. This is particularly important as the groups of people with access to each environment are usually not the same.\n",
      "\n",
      "R12 Isolate each phase of the AI system into a dedicated environment\n",
      "\n",
      "The 3 technical environments corresponding to each phase of the AI system’s lifecycle should be siloed. This isolation may involve:\n",
      "\n",
      "- Network isolation: each environment is integrated into a physically or logically dedicated network;\n",
      "\n",
      "- System isolation: each environment has its own dedicated physical servers or hypervisors;\n",
      "\n",
      "- Storage isolation: each environment has its own storage hardware or dedicated disks. At the very least, there will be a logical segmentation;\n",
      "\n",
      "- Accounts and secrets isolation: each environment has its own users and administrators accounts and separate credentials.\n",
      "\n",
      "For AI systems which are hosted on the Internet, it is recommended to follow the ANSSI recommendations for the design of a secure Internet gateway [25].\n",
      "\n",
      "R13 Implement a secure Internet gateway for an AI system hosted on the Internet\n",
      "\n",
      "For AI systems which are hosted on the Internet, it is recommended to follow the isolation best practices of the ANSSI guide on this subject, in particular:\n",
      "\n",
      "- implement a reverse-proxy function before accessing the AI system web service;\n",
      "\n",
      "Processing page 21...\n",
      "Page 21 OCR result:\n",
      "■ set up two logical areas for network filtering using firewalls: external filtering on the Internet front end and internal filtering before accessing the AI system;\n",
      "\n",
      "■ do not use any of the entity’s internal directories for authentication on the AI system;\n",
      "\n",
      "■ avoid mutualisation of security functions on the same hypervisor in the secure Internet gateway (firewalls, reverse-proxy, logging server, etc.).\n",
      "\n",
      "If the entity choose a public cloud 15 to host its service and security requirements make this necessary, a SecNumCloud [33] qualified service provider should be chosen.\n",
      "\n",
      "R14 Prioritise SecNumCloud hosting when deploying an AI system in a public cloud\n",
      "\n",
      "If the organisation chooses to use public Cloud hosting, it is recommended that a trusted SecNumCloud offer be used in the following cases:\n",
      "\n",
      "■ The data processed by the AI system is considered sensitive;\n",
      "\n",
      "■ The impact of the AI system on the business is considered critical;\n",
      "\n",
      "■ AI system users are not considered trusted.\n",
      "\n",
      "When designing the project, a downgraded mode must be systematically implemented without any AI in order to meet business needs if the AI system is unavailable or fails.\n",
      "\n",
      "R15 Provide a downgraded version of business services without an AI system\n",
      "\n",
      "To prevent malfunctions or inconsistencies in the responses provided by the AI model, it is recommended that, at minimum, there is an AI system bypass procedure for users, in order to meet business needs.\n",
      "\n",
      "The deployment of generative AI and LLM systems generally involves GPUs 16 to enhance system performance, whether in the training or production phase.\n",
      "\n",
      "These GPUs may process sensitive data linked to the AI model’s operations. To protect against data leaks, these GPU hardware components should be dedicated to the AI system and not be shared with other IS business applications. GPUs, on the other hand, can be shared between several AI models, but only if they have the same level of sensitivity and same security requirements.\n",
      "\n",
      "R16 Dedicate GPU components to the AI system\n",
      "\n",
      "Physical GPU components should be dedicated to the processing carried out by the AI system. In the case of virtualisation, hypervisors with access to GPU cards should be dedicated to the AI system, or at least have a hardware filtering function (e.g.: IOMMU 17) to restrict virtual machine access to the memory on these GPU cards.\n",
      "\n",
      "15. A public cloud is a hosting service shared between several customers and hosted on the Internet.\n",
      "16. Graphics processing units\n",
      "17. Input-Output Memory Management Unit\n",
      "\n",
      "Processing page 22...\n",
      "Page 22 OCR result:\n",
      "Like most business applications, AI systems can be subject to side-channel attacks. These attacks usually aim to exfiltrate sensitive information or disrupt the proper functioning of AI systems. While most of these attacks are not specific to an AI system, some may nevertheless rely on mechanisms that are specific to generative AI systems.\n",
      "\n",
      "R17 Manage side-channel attacks on the AI system\n",
      "\n",
      "It is recommended to ensure that the AI system is not vulnerable to attacks via auxiliary channels (time, energy consumption, etc.) which could, for example, enable an attacker to rebuild a response provided by an AI model.\n",
      "\n",
      "5.2 Recommendations for the training phase\n",
      "\n",
      "The issue of data confidentiality has already been covered in a previous general recommendation (see R7). In particular, and regarding the number of vulnerabilities published on generative AI tools, it should be assumed that a user with access to a trained AI model could potentially have access to the training data for this model.\n",
      "\n",
      "To reduce the risks associated with the confidentiality of training data, it is sometimes necessary to use an anonymisation process or generate a synthetic dataset from the original raw data. In some cases, these measures can resolve the issue of protecting information, but it is nevertheless important to be vigilant about attacks aimed at retrieving the initial information from anonymised or synthetic data 18: attacks by attribute or membership inference, re-identification based on cross-referencing with other datasets, etc.\n",
      "\n",
      "R18 Train an AI model only with data which users can legitimately access\n",
      "\n",
      "It is strongly recommended to train a model with data of a level of sensitivity consistent with the users’ access rights.\n",
      "\n",
      "As we saw earlier, attacks specifically targeting the training phase of a model are possible, such as the injection of malicious data into training datasets, or modifying certain data to cause the model to malfunction once it has been deployed in production.\n",
      "\n",
      "R19 Protect the integrity of AI model training data\n",
      "\n",
      "The integrity of the model’s training data should be ensured throughout the training cycle. This protection may take the form of systematic checking the signature or hash of the files used (or compressed archives with all this data).\n",
      "\n",
      "17. Input–output memory management unit\n",
      "18. see, for example, https://cdn.arstechnica.net/wp-content/uploads/2024/03/LLM-Side-Channel.pdf\n",
      "18. Refer to the CNIL’s work on this subject: https://linc.cnil.fr/donnees-synthetiques-et-lhomme-crea-les-donnees-son-image-22.\n",
      "\n",
      "Processing page 23...\n",
      "Page 23 OCR result:\n",
      "### Protect the integrity of AI system files\n",
      "\n",
      "**R20** The integrity of the trained model files should be protected and regular checks done to ensure that these have not been altered. This recommendation also applies by extension to all the files inherent to the proper working of the AI system (scripts, binaries, etc.).\n",
      "\n",
      "In the majority of use cases, a trained AI model does not need to be subject to regular modifications or adjustment of its parameters. If a malfunction is detected, or when optimising the model's performance, it is preferable to carry out re-training operations using the dedicated training environment.\n",
      "\n",
      "To respect this, continuous learning methods, also known as online learning (where the model learns in real-time from the data sent as input), should be avoided as far as possible. Using offline learning methods, based on selected and tested data sets, reduces the risk of the model malfunctioning or being poisoned.\n",
      "\n",
      "An AI model can be re-trained on a recurring and fixed basis (e.g. every month), triggered when a performance gap crosses a given threshold or when the data used to train the model are no longer relevant, or on demand on an ad-hoc basis.\n",
      "\n",
      "**R21** **Do not re-train an AI model in production**\n",
      "\n",
      "It is strongly recommended not to re-train an AI model directly in production. Re-training should start with the 3-phases cycle, in the suitable environments for each phase.\n",
      "\n",
      "### 5.3 Recommendations for the deployment phase\n",
      "\n",
      "The deployment of a generative AI system must be based on a secure deployment environment based, for example, on fully harnessed and robust CI/CD chains.\n",
      "\n",
      "These CI/CD chains must be operated from an administration system and from dedicated, robust administrator workstations.\n",
      "\n",
      "**R22** **Secure the production deployment chain for AI systems**\n",
      "\n",
      "It is recommended that generative AI systems be deployed from an administrative IS, in compliance with ANSSI’s secure administration best practices guide [21].\n",
      "\n",
      "A security audit should be carried out by specialised teams which are trained in the specifics of AI systems. This phase must take place before deployment to production in order to test the vulnerabilities inherent to AI systems (adversarial attacks, etc.).\n",
      "\n",
      "Processing page 24...\n",
      "Page 24 OCR result:\n",
      "### Conduct security audits of AI systems before deployment to production\n",
      "\n",
      "Robustness and security tests of AI systems are recommended. These tests can be:\n",
      "\n",
      "- Standard penetration tests on the usual technical components of an AI system: web servers, orchestrator, database, etc. ;\n",
      "\n",
      "- Security tests on developments made in the AI system (using SAST or DAST tools, for example);\n",
      "\n",
      "- Automated tests specifically targeting vulnerabilities related to AI models (adversarial attacks, model extraction, etc.);\n",
      "\n",
      "- Manual auditor tests specifically aimed at testing the robustness of a generative AI model in more sophisticated attack scenarios.\n",
      "\n",
      "To carry out safety audits on a generative AI system, it is possible to use ANSSI-qualified PASSI [31] service providers.\n",
      "\n",
      "### Conduct business tests of AI systems before deployment to production\n",
      "\n",
      "Performance and quality tests should be carried out on the answers provided by a generative AI system.\n",
      "\n",
      "### Information\n",
      "\n",
      "Functional testing of the AI system can take place continuously at a given frequency and not only during deployment. This can be used to detect model malfunctions at an early stage and correct them more reactively.\n",
      "\n",
      "### 5.4 Recommendations for the production phase\n",
      "\n",
      "As mentioned earlier, it is difficult to apply the restrictions for user right access to the training data of a model, which may be subject to attacks aiming to extract this data by querying the model (exfiltration).\n",
      "\n",
      "Similarly, some malicious requests may be aimed at hijacking the generative AI service, for example by inducing hallucinations or incorrect responses.\n",
      "\n",
      "As part of a defence-in-depth approach, it is important to explore the possibility of detecting or blocking some malicious queries intended, for example, to extract the model data or additional data (this additional data may include user inputs in certain cases).\n",
      "\n",
      "This protection can also be useful in reducing the risk of the model leak. If the model has been trained on sensitive data, the model parameters leak can lead to some of this data leak as a result of some attacks (e.g. model inversion attack or membership inference attack). As such, responses to users must be as simple as possible (character strings only) and must not return any prediction score or other internal model mechanisms.\n",
      "\n",
      "19. There are a number of specialised tools available, such as https://github.com/microsoft/responsible-ai-toolbox, https://github.com/Trusted-AI/adversarial-robustness-toolbox, or https://github.com/protectai/ai-exploits.\n",
      "\n",
      "Processing page 25...\n",
      "Page 25 OCR result:\n",
      "Finally, depending on the use case, it may be appropriate to define a limit to the size of the answers provided by the AI model. This can reduce the risk of data leak.\n",
      "\n",
      "**R25 Protect the AI system by filtering user input and output**\n",
      "\n",
      "These functions should be implemented to protect against data leak or model leak in responses:\n",
      "\n",
      "- A function to filter malicious user queries before these are sent to the model;\n",
      "- A filter function for queries deemed to be non-legitimate from a business point of view;\n",
      "- A filter function for internal model information (parameters, training) in the responses;\n",
      "- A filter function for information defined as sensitive in responses (e.g. private details, project references, etc.);\n",
      "- A limit on the size of responses (maximum number of characters).\n",
      "\n",
      "The AI system’s interaction with other business applications or other IS technical resources can be source of vulnerabilities.\n",
      "\n",
      "These interactions often take the form of plugins offered by AI model editors. These plugins will enable the AI system to be interconnected with bureaucratic tools and social networks, or potentially critical infrastructure components (identity management, network resources, etc.).\n",
      "\n",
      "These interactions can also facilitate the lateralisation of an attacker on the IS, if he takes advantage of a vulnerability in the AI system.\n",
      "\n",
      "The literature on this topic often refers to the risks of indirect prompt injection and the problems that may result from sending uncontrolled data to a generative AI model (for example, the content of a received email or a web page resulting from a search). This kind of use is more problematic when an action is carried out without human validation (see recommendation R9).\n",
      "\n",
      "It is therefore essential to be able to control the interaction of the AI system with other IS resources.\n",
      "\n",
      "**R26 Manage and secure the interactions of the AI system with other business applications**\n",
      "\n",
      "All the interactions and network flows of the AI system must be documented and approved. Network flows between the AI system and other resources must comply with the state-of-the-art in terms of security:\n",
      "\n",
      "- They must be strictly filtered at network level, encrypted and authenticated (e.g. by following the ANSSI TLS [19] guide);\n",
      "- They must use secure protocols (e.g. OpenID Connect) when using an identity provider [24];\n",
      "\n",
      "20. see the article “Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection” for more details: https://arxiv.org/abs/2302.12173\n",
      "\n",
      "Processing page 26...\n",
      "Page 26 OCR result:\n",
      "■ In addition to authentication, the authorisation of access to the resource must also be checked;\n",
      "\n",
      "■ They must be logged at the appropriate level of granularity.\n",
      "\n",
      "R27 Limit automatic actions performed by an AI system handling uncontrolled inputs\n",
      "\n",
      "It is strongly recommended that automated actions on the IS be limited or even prohibited when these are triggered by an AI system and uncontrolled inputs (e.g. data from the Internet or emails, etc.).\n",
      "\n",
      "Depending on the use case of the AI system and its criticality from a business point of view, it may be appropriate to deploy it in one or more dedicated environments, not shared with other business applications.\n",
      "\n",
      "R28 Isolate the AI system in one or more dedicated technical environments\n",
      "\n",
      "It is recommended that the AI system be siloed into dedicated logical zones, in order to limit the risk of an attacker who has compromised the system moving laterally.\n",
      "\n",
      "Actions on the AI system must be logged with adequate level of information granularity, in particular regarding the inputs and outputs of the AI model.\n",
      "\n",
      "For the purposes of traceability and explicability of the AI system, it is important to make a clear distinction between the queries made by users and the data actually sent to the AI model. In fact, for performance and security reasons, user queries may be subject to specific pre-processing and formatting before being sent to the model.\n",
      "\n",
      "These two pieces of information are crucial in facilitating the management of an incident and must be traceable in the AI system’s application logs. The aim is to be able to fully rebuild an event on the AI system, when detecting a malicious query, for example.\n",
      "\n",
      "Regarding the architecture of a logging system, refer to the ANSSI’s general guide on this subject [27].\n",
      "\n",
      "R29 Record all processing carried out within the AI system\n",
      "\n",
      "All processing carried out on the AI system should be logged at the correct level of granularity, in particular:\n",
      "\n",
      "■ User queries (ensuring these are secured if they contain sensitive data);\n",
      "\n",
      "■ The input processing carried out on this query before it is sent to the model;\n",
      "\n",
      "■ Calls to plugins;\n",
      "\n",
      "■ Calls for additional data;\n",
      "\n",
      "■ The processing carried out by the output filters;\n",
      "\n",
      "Processing page 27...\n",
      "Page 27 OCR result:\n",
      "■ Responses to users.\n",
      "\n",
      "Warning\n",
      "\n",
      "User data logging must comply with CNIL [9] requirements concerning the protection of personal data (GDPR) and in particular for how long is this data kept on the AI system.\n",
      "\n",
      "5.5 Special case of AI-assisted source code generation\n",
      "\n",
      "Generative AI tools can be specialised and specifically trained to generate source code in a lot of programming languages.\n",
      "\n",
      "These resources can help developers save time, but there are also risks regarding the quality of the code (pushing vulnerabilities) or the insertion of backdoors if an attacker has compromised the AI model.\n",
      "\n",
      "It is therefore important to be careful about the source code generated by AI.\n",
      "\n",
      "R30 Check AI-generated source code systematically\n",
      "\n",
      "The source code generated by AI must be subject to security measures to ensure that it is not harmful:\n",
      "\n",
      "■ Prohibition of automatic execution of AI-generated source code in the development environment;\n",
      "\n",
      "■ Prohibition of automatic commit of AI-generated source code to repositories;\n",
      "\n",
      "■ Integration of an AI-generated source code remediation tool [3, 4] into the development environment;\n",
      "\n",
      "■ Check that the libraries referenced in the result of the source code generated by AI are harmless;\n",
      "\n",
      "■ The quality of the source code generated from sufficiently sophisticated standard queries needs to be checked regularly by a human.\n",
      "\n",
      "R31 Limit AI source code generation for critical application modules\n",
      "\n",
      "It is strongly recommended that generative AI tools are not used to generate blocks 21 of source code intended for critical modules:\n",
      "\n",
      "■ Cryptography modules (authentication, encryption, signatures, etc.);\n",
      "\n",
      "■ User and administrator access rights management modules;\n",
      "\n",
      "■ Sensitive data processing modules.\n",
      "\n",
      "21. A block here refers to a complete set of instructions in the source code, for example the complete definition of a function, a procedure, an object class, a shell script, etc.\n",
      "\n",
      "Processing page 28...\n",
      "Page 28 OCR result:\n",
      "### R32 Raise developers awareness of the risks associated with AI-generated source code\n",
      "\n",
      "Awareness-raising campaigns on the risks associated with the use of AI-generated source code should be carried out. This awareness can be based on public reports on the subject or research papers demonstrating the presence of vulnerabilities in AI-generated code. In addition, developers can also be trained in AI tools to optimise their queries (prompt engineering) to improve the quality and security of the generated code.\n",
      "\n",
      "#### Information\n",
      "\n",
      "Depending on the use case, it may also be appropriate to specifically train a model (alignment stage) so that it cannot generate deliberately malicious code.\n",
      "\n",
      "### 5.6 Special case of consumer AI services hosted on the Internet\n",
      "\n",
      "If the entity wishes to offer a service based on generative AI to the public, specific care must be taken to ensure the security of this service, given its high exposure.\n",
      "\n",
      "An additional threat to consider during the risk analysis is the potential of damage to the entity’s reputation.\n",
      "\n",
      "### R33 Strengthen security measures for AI services hosted on the Internet\n",
      "\n",
      "Specific attention should be paid to certain security measures for services available to the public, in particular:\n",
      "\n",
      "- Training the AI model using only publicly available data;\n",
      "- Ensuring that users of the AI system have been authenticated before use;\n",
      "- Systematic analysis of user queries on the AI system;\n",
      "- Checking and validating responses before these are sent to users;\n",
      "- Protecting the confidentiality of user data (history of queries and responses, etc.);\n",
      "- Implementing measures against distributed denial of service (DDoS) attacks;\n",
      "- Securing the web service at the user front end.\n",
      "\n",
      "### 5.7 Special case of using third-party generative AI solutions\n",
      "\n",
      "In this final section, the guide goes over the case where the entity is not managing a generative AI service but is a customer of a third-party generative AI service (see responsibility-sharing scenarios\n",
      "\n",
      "23. The reports by Snyk (https://snyk.io/fr/reports/ai-code-security/) could be cited as an example, or the research carried out by Stanford University on this subject (https://arxiv.org/pdf/2211.03622.pdf).\n",
      "\n",
      "24. For example, an initial AI code generation query can be combined with by a static analysis test of this code and then a second query asking the AI to treat the vulnerabilities detected in the original generated code.\n",
      "\n",
      "Processing page 29...\n",
      "Page 29 OCR result:\n",
      "in chapter 2). The purpose of this last section is to restate the essential points which must be observed and taken into account by users of these third-party services.\n",
      "\n",
      "Because they are so easy to use, it’s tempting to use generative AI tools that are available on the Internet to process business data, for example for text translation. Sending information (text, images, documents) to a generative AI service which is available to the public is the same as pushing the information on a storage space belonging to editors.\n",
      "\n",
      "Isolation between clients and protection of the confidentiality of data sent to an Internet AI system are not always in the state-of-the-art and rely solely on trust in the service provider. In this regard, it is important to note that with the majority of services, the data sent to the service is collected and used by the service provider to optimise the models24.\n",
      "\n",
      "Therefore, sensitive data must absolutely not be sent to third-party generative AI services such as ChatGPT, Gemini, Copilot, DeepL (text translation) or Perplexity, to name only the most popular services. The relevant data includes:\n",
      "\n",
      "- French Diffusion Restreinte data [30] or classified data [32];\n",
      "\n",
      "- Research work relating to French PPST [20];\n",
      "\n",
      "- Personal data (private information, contact details, etc.);\n",
      "\n",
      "- The company’s contractual, legal and financial data;\n",
      "\n",
      "- IT secrets, such as passwords or authentication tokens (API keys).\n",
      "\n",
      "R34 Do not use generative AI tools on the Internet for professional use involving sensitive data\n",
      "\n",
      "As the client entity does not control the generative AI service, it is impossible to ensure that the confidentiality of data submitted for input meets the entity’s security requirements.\n",
      "As a precautionary measure, it is therefore essential never to include any of the entity’s sensitive data in user queries.\n",
      "\n",
      "Warning\n",
      "\n",
      "This recommendation also concerns the use of generative AI tools to generate synthetic datasets for training or fine-tuning an AI model.\n",
      "\n",
      "Some third-party generative AI tools offer connections with office automation tools or standard business applications on the Internet. Particular attention must be paid to the configuration of generative AI tools’ rights of access to the entity’s business data: emails, documents areas, source code repositories, audio and video conferencing services, etc.\n",
      "\n",
      "24. see ChatGPT’s usage policy for example: https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-are-developed\n",
      "\n",
      "Processing page 30...\n",
      "Page 30 OCR result:\n",
      "R35 Perform regular reviews of the configuration of rights for generative AI tools on business applications\n",
      "\n",
      "A review of access rights for generative AI tools should be carried out as soon as the entity activates the product to ensure that the rights set by default are not too low or open by design.\n",
      "Finally, access rights must be reviewed on a regular basis (e.g. every month), to ensure that the product’s functional and security updates have no impact on user information access rights.\n",
      "\n",
      "28 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 31...\n",
      "Page 31 OCR result:\n",
      "Recommendation List\n",
      "\n",
      "R1 Integrate security into all phases of the lifecycle of an AI system 9\n",
      "R2 Conduct a risk analysis on AI systems before the training phase 13\n",
      "R3 Evaluate the level of confidence in the libraries and external modules used in the AI system 14\n",
      "R4 Evaluate the level of confidence in external data sources used in the AI system 14\n",
      "R5 Apply DevSecOps principles to all phases of the project 15\n",
      "R6 Use secure AI model formats 15\n",
      "R7 Manage data confidentiality issues from the AI system design phase 16\n",
      "R8 Manage users data access rights issue from the AI system design phase 16\n",
      "R9 Do not allow AI systems to run automatically critical actions on the IS 17\n",
      "R10 Manage and secure developer and administrator privileged access to the AI system 17\n",
      "R11 Host the AI system in trusted environments consistent with security needs 18\n",
      "R12 Isolate each phase of the AI system into a dedicated environment 18\n",
      "R13 Implement a secure Internet gateway for an AI system hosted on the Internet 19\n",
      "R14 Prioritise SecNumCloud hosting when deploying an AI system in a public cloud 19\n",
      "R15 Provide a downgraded version of business services without an AI system 19\n",
      "R16 Dedicate GPU components to the AI system 19\n",
      "R17 Manage side-channel attacks on the AI system 20\n",
      "R18 Train an AI model only with data which users can legitimately access 20\n",
      "R19 Protect the integrity of AI model training data 20\n",
      "R20 Protect the integrity of AI system files 21\n",
      "R21 Do not re-train an AI model in production 21\n",
      "R22 Secure the production deployment chain for AI systems 21\n",
      "R23 Conduct security audits of AI systems before deployment to production 22\n",
      "R24 Conduct business tests of AI systems before deployment to production 22\n",
      "R25 Protect the AI system by filtering user input and output 23\n",
      "R26 Manage and secure the interactions of the AI system with other business applications 24\n",
      "R27 Limit automatic actions performed by an AI system handling uncontrolled inputs 24\n",
      "R28 Isolate the AI system in one or more dedicated technical environments 24\n",
      "R29 Record all processing carried out within the AI system 25\n",
      "R30 Check AI-generated source code systematically 25\n",
      "R31 Limit AI source code generation for critical application modules 25\n",
      "R32 Raise developers awareness of the risks associated with AI-generated source code 26\n",
      "R33 Strengthen security measures for AI services hosted on the Internet 26\n",
      "R34 Do not use generative AI tools on the Internet for professional use involving sensitive data 27\n",
      "R35 Perform regular reviews of the configuration of rights for generative AI tools on business applications 28\n",
      "\n",
      "Processing page 32...\n",
      "Page 32 OCR result:\n",
      "# Bibliography\n",
      "\n",
      "[1] BSI - Artificial Intelligence.\n",
      "Site institutionnel, BSI.\n",
      "https://www.bsi.bund.de/EN/Themen/Unternehmen-und-Organisationen/Informationen-und-Empfehlungen/Kuenstliche-Intelligenz/kuenstliche-intelligenz_node.html.\n",
      "\n",
      "[2] CNIL - Intelligence artificielle (IA).\n",
      "Site institutionnel, CNIL.\n",
      "https://www.cnil.fr/fr/intelligence-artificielle-ia.\n",
      "\n",
      "[3] NIST - Source Code Security Analyzers.\n",
      "Site institutionnel, NIST.\n",
      "https://www.nist.gov/itl/ssd/software-quality-group/source-code-security-analyzers.\n",
      "\n",
      "[4] OWASP - Source Code Analysis Tools.\n",
      "Technical report, OWASP.\n",
      "https://owasp.org/www-community/Source_Code_Analysis_Tools.\n",
      "\n",
      "[5] NCSC-UK - Secure development and deployment guidance.\n",
      "Site institutionnel, NCSC-UK, novembre 2018.\n",
      "https://www.ncsc.gov.uk/collection/developers-collection.\n",
      "\n",
      "[6] ENISA - Artificial Intelligence Cybersecurity Challenges.\n",
      "Site institutionnel, ENISA, décembre 2020.\n",
      "https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges.\n",
      "\n",
      "[7] ENISA - Securing Machine Learning Algorithms.\n",
      "Site institutionnel, ENISA, décembre 2021.\n",
      "https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms.\n",
      "\n",
      "[8] CISA - Securing the software supply chain.\n",
      "Site institutionnel, CISA, août 2022.\n",
      "https://media.defense.gov/2022/Sep/01/2003068942/-1/-1/0/ESF_SECURING_THE_SOFTWARE_SUPPLY_CHAIN_DEVELOPERS.PDF.\n",
      "\n",
      "[9] CNIL - IA : comment être en conformité avec le RGPD ?\n",
      "Site institutionnel, CNIL, avril 2022.\n",
      "https://www.cnil.fr/fr/intelligence-artificielle/ia-comment-etre-en-conformite-avec-le-rgpd.\n",
      "\n",
      "[10] NIST - Secure Software Development Framework (SSDF) Version 1.1: Recommendations for Mitigating the Risk of Software Vulnerabilities.\n",
      "Site institutionnel, NIST, février 2022.\n",
      "https://csrc.nist.gov/pubs/sp/800/218/final.\n",
      "\n",
      "Processing page 33...\n",
      "Page 33 OCR result:\n",
      "[11] CISA - Defending Continuous Integration/Continuous Delivery (CI/CD) Environments. Site institutionnel, CISA, juin 2023. https://media.defense.gov/2023/Jun/28/2003249466/-1/-1/0/CSI_DEFENDING_CI_CD_ENVIRONMENTS.PDF.\n",
      "\n",
      "[12] DINUM - Le Cloud pour les administrations. Site institutionnel, DINUM, Mai 2023. https://www.numerique.gouv.fr/services/cloud/regles-doctrine/#contenu.\n",
      "\n",
      "[13] Doctrine d’utilisation de l’informatique en nuage par l’État - Cloud au centre. Site institutionnel, LEGIFRANCE, Mai 2023. https://www.legifrance.gouv.fr/download/pdf/circ?id=45446.\n",
      "\n",
      "[14] NCSC-UK - Guidelines for secure AI system development. Site institutionnel, NCSC-UK, novembre 2023. https://www.ncsc.gov.uk/collection/guidelines-secure-ai-system-development.\n",
      "\n",
      "[15] NIST - Artificial Intelligence Risk Management Framework. Site institutionnel, NIST, janvier 2023. https://www.nist.gov/itl/ai-risk-management-framework.\n",
      "\n",
      "[16] NIST - Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations. Site institutionnel, NIST, janvier 2024. https://csrc.nist.gov/pubs/ai/100/2/e2023/final.\n",
      "\n",
      "[17] Guideline for a healthy information system. Guide ANSSI-GP-042-EN v2.0, ANSSI, septembre 2017. https://cyber.gouv.fr/en/publications/guideline-healthy-information-system-42-measures.\n",
      "\n",
      "[18] Comprendre et anticiper les attaques DDoS. Guide Version 1.0, ANSSI, mars 2015. https://cyber.gouv.fr/guide-ddos.\n",
      "\n",
      "[19] Security recommendations for TLS. Guide SDE-NT-035-EN v1.1, ANSSI, janvier 2017. https://cyber.gouv.fr/en/publications/security-recommendations-tls.\n",
      "\n",
      "[20] Protection du potentiel scientifique et technique de la nation. Guide ANSSI-PA-049 v1.0, ANSSI, avril 2018. https://cyber.gouv.fr/guide-zrr.\n",
      "\n",
      "[21] Recommendations to secure administration of IT systems. Guide ANSSI-PA-022-EN v2.0, ANSSI, avril 2018. https://cyber.gouv.fr/en/publications/recommendations-secure-administration-it-systems.\n",
      "\n",
      "[22] Controlling the digital risk - The trust advantage. Guide ANSSI-PA-070-EN v1.0, ANSSI, novembre 2019. https://cyber.gouv.fr/en/publications/controlling-digital-risk-trust-advantage.\n",
      "\n",
      "Processing page 34...\n",
      "Page 34 OCR result:\n",
      "[23] EBIOS Risk Manager - Methodological sheets.\n",
      "Guide ANSSI-PA-058-EN v1.0, ANSSI, novembre 2019.\n",
      "https://cyber.gouv.fr/en/digital-risk-management.\n",
      "\n",
      "[24] Recommandations pour la sécurisation de la mise en œuvre du protocole OpenID Connect.\n",
      "Guide ANSSI-PA-080 v1.0, ANSSI, septembre 2020.\n",
      "https://cyber.gouv.fr/guide-oidc.\n",
      "\n",
      "[25] Recommandations relatives à l’interconnexion d’un système d’information à Internet.\n",
      "Guide ANSSI-PA-066 v3.0, ANSSI, juin 2020.\n",
      "https://cyber.gouv.fr/guide-interconnexion-si-internet.\n",
      "\n",
      "[26] Recommandations pour la mise en œuvre d’un site Web : maîtriser les standards de sécurité côté navigateur.\n",
      "Guide ANSSI-PA-009 v2.1, ANSSI, avril 2021.\n",
      "https://cyber.gouv.fr/guide-sites-web.\n",
      "\n",
      "[27] Recommandations de sécurité pour l’architecture d’un système de journalisation.\n",
      "Guide DAT-PA-012 v2.0, ANSSI, janvier 2022.\n",
      "https://cyber.gouv.fr/guide-journalisation.\n",
      "\n",
      "[28] Les essentiels - DevSecOps.\n",
      "Guide Version 1.0, ANSSI, février 2024.\n",
      "https://cyber.gouv.fr/publications/devsecops.\n",
      "\n",
      "[29] Recommandations de sécurité pour un système d’IA générative.\n",
      "Guide ANSSI-PA-102 v1.0, ANSSI, avril 2024.\n",
      "https://cyber.gouv.fr/guide-ia-generative.\n",
      "\n",
      "[30] Instruction interministérielle n°901.\n",
      "Référentiel Version 1.0, ANSSI, janvier 2015.\n",
      "https://cyber.gouv.fr/ii901.\n",
      "\n",
      "[31] Prestataires d’audit de la sécurité des systèmes d’information. Référentiel d’exigences.\n",
      "Référentiel Version 2.1, ANSSI, octobre 2015.\n",
      "https://cyber.gouv.fr/referentiels-dexigences-pour-la-qualification.\n",
      "\n",
      "[32] Instruction générale interministérielle n°1300.\n",
      "Référentiel, SGDSN, août 2021.\n",
      "https://cyber.gouv.fr/igi1300.\n",
      "\n",
      "[33] Prestataires de services d’informatique en nuage (SecNumCloud). Référentiel d’exigences.\n",
      "Référentiel Version 3.2, ANSSI, mars 2022.\n",
      "https://cyber.gouv.fr/secnumcloud.\n",
      "\n",
      "32 – SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM\n",
      "\n",
      "Processing page 35...\n",
      "Page 35 OCR result:\n",
      "blank page\n",
      "\n",
      "SECURITY RECOMMENDATIONS FOR A GENERATIVE AI SYSTEM - 33\n",
      "\n",
      "Processing page 36...\n",
      "Page 36 OCR result:\n",
      "blank page\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_path = \"/PATH/TO/PDF.pdf\"\n",
    "images = pdf_to_images(pdf_path, dpi=200)\n",
    "api_key = \"***\"\n",
    "output_file = \"ocr_result.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for page_idx, img in enumerate(images, start=1):\n",
    "        print(f\"Processing page {page_idx}...\")\n",
    "        base64_img = image_to_base64(img)\n",
    "\n",
    "        question = f\"Please perform OCR on this image. Only give the raw content, do not include boilerplate in your answer such as 'Certainly, here it is' or 'Yes, here you go'. If the page is blank, return 'blank page'.\"\n",
    "        try:\n",
    "            response_json = call_albert_api_with_image(base64_img, api_key, question=question)\n",
    "\n",
    "            # Extract the text from the response (if the model is returning OCR text in 'message.content')\n",
    "            # That depends on how the model structures OCR output\n",
    "            if \"choices\" in response_json and len(response_json[\"choices\"]) > 0:\n",
    "                content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "                print(f\"Page {page_idx} OCR result:\\n{content}\\n\")\n",
    "                f.write(f\"{content}\\n\\f\")\n",
    "            else:\n",
    "                print(f\"Page {page_idx}: Unexpected response:\\n{response_json}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error OCRing page {page_idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da063f82-1d9a-4cd1-93d6-0d89666baac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-albert-api",
   "language": "python",
   "name": "env-albert-api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
