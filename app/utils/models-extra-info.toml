["DeepSeek-R1"]
url = "https://huggingface.co/deepseek-ai/DeepSeek-R1"
simple_name = "DeepSeek R1"
organisation = "DeepSeek"
total_params = 671
active_params = 37
icon_path = "deepseek.webp"
license = "MIT"
excerpt = "Sorti en janvier 2025, ce modèle est basé sur Deepseek V3 qui élabore un raisonnement avant de formuler une réponse."
description = "Sorti en janvier 2025, ce modèle phare de la société chinoise DeepSeek, est un modèle qui élabore un raisonnement avant de formuler une réponse. DeepSeek R1 est basé sur Deepseek V3, qui possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
reasoning = true
release_date = "01/2025"

["deepseek-r1-distill-llama-70b"]
url = "https://huggingface.co/deepseek-ai/deepseek-r1-distill-llama-70b"
simple_name = "DeepSeek R1 Llama 70B"
organisation = "DeepSeek"
params = 70
icon_path = "deepseek.webp"
excerpt = "Ce modèle est un modèle Llama 3.3 70B transformé par l'entreprise DeepSeek pour être doté d'un système de raisonnement à travers un corpus synthétique généré par DeepSeek R1."
description = "Ce modèle est un modèle Llama 3.3 70B transformé par l'entreprise DeepSeek pour être doté d'un système de raisonnement à travers un corpus synthétique généré par DeepSeek R1. Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
license = "Llama 3.3"
reasoning = true

["deepseek-ai/DeepSeek-V3"]
url = "https://huggingface.co/deepseek-ai/DeepSeek-V3"
simple_name = "DeepSeek v3"
organisation = "DeepSeek"
total_params = 671
active_params = 37
icon_path = "deepseek.webp"
license = "MIT"
excerpt = "Sorti en décembre 2024, le modèle DeepSeek V3 possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
description = "Sorti en décembre 2024, ce modèle phare de la société chinoise DeepSeek possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
release_date = "12/2024"


["gemma-2-27b-it"]
url = "https://huggingface.co/google/gemma-2-27b-it"
simple_name = "Gemma 2 27B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 27
excerpt = "Modèle performant avec une taille correcte, son coût relativement élevé le destine à des usages spécifiques nécessitant une grande précision."
description = "Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions. Le modèle sollicité ici est la version quantisée (q8)."

["Gemma-2-9B-it"]
url = "https://huggingface.co/google/gemma-2-9b-it"
simple_name = "Gemma 2 9B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 9
excerpt = "Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."
description = "Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."
release_date = "06/2024"

["gemma-2-2b-it"]
url = "https://huggingface.co/google/gemma-2-2b-it"
simple_name = "Gemma 2 2B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 2
description = "Petit frère de la famille Gemma 2, ce très petit modèle sorti en juillet 2024 arrive à rivaliser avec des modèles bien plus gros."
release_date = "07/2024"

["gemma-3-12b-it"]
url = "https://huggingface.co/google/gemma-3-12b-it"
simple_name = "Gemma 3 12B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 12
excerpt = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et sa taille lui permet de fonctionner sur des cartes graphiques à usage non professionnel."
description = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et sa taille lui permet de fonctionner sur des cartes graphiques à usage non professionnel."
release_date = "03/2025"

["gemma-3-4b-it"]
url = "https://huggingface.co/google/gemma-3-4b-it"
simple_name = "Gemma 3 4B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 4
excerpt = "Troisième génération de la famille Gemma de Google, sa toute petite taille lui permet de fonctionner sur des petites cartes graphiques voire sur téléphone portable."
description = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et sa taille lui permet de fonctionner sur des petites cartes graphiques voire sur téléphone portable."
release_date = "03/2025"

["mistral-small-3.1-24b-instruct-2503"]
url = "https://huggingface.co/mistralai/mistral-small-3.1-24b-instruct-2503"
simple_name = "Mistral Small 3.1 24B"
organisation = "Mistral AI"
icon_path = "mistral.png"
license = "Mistral"
params = 24
excerpt = "Mistral Small 3.1 24B Instruct est une variante améliorée du Mistral Small 3 (janvier 2025), dotée de 24 milliards de paramètres et de capacités multimodales avancées."
description = "Mistral Small 3.1 24B Instruct est un modèle multimodal qui offre des performances de pointe dans les tâches de raisonnement basées sur le texte et la vision, y compris l'analyse d'images, la programmation, le raisonnement mathématique et le soutien multilingue pour des dizaines de langues."
release_date = "03/2025"

["gemma-3-27b-it"]
url = "https://huggingface.co/google/gemma-3-27b-it"
simple_name = "Gemma 3 27B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 27
excerpt = "Modèle de troisième génération de la famille Gemma de Google. Malgré sa moyenne taille, il pouvait rivaliser à sa sortie avec de nombreux modèles propriétaires supposés bien plus gros."
description = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et malgré sa moyenne taille, il pouvait rivaliser à sa sortie avec de nombreux modèles propriétaires supposés bien plus gros."
release_date = "03/2025"

["Hermes-3-Llama-3.1-405B"]
url = "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B"
simple_name = "Hermes 3 405B"
organisation = "Nous"
icon_path = "nous.webp"
params = 405
excerpt = "Sorti en juillet 2024, ce modèle est le plus grand modèle de la série Hermes. C'est une adaptation du modèle Llama 3.1 405B de Meta."
description = "Doté de 405 milliards de paramètres, ce modèle est la version la plus large de la troisième itération des modèles Hermes. Ce modèle Hermes 3 est un fine-tune conçu par l'entreprise américaine 'Nous Research' à partir du modèle Llama 3.1 405B de Meta. Sa taille et son coût élevé en inférence le réservent à des applications exigeantes de raisonnement et de programmation."
license = "Llama 3.1"
release_date = "07/2024"

["meta-Llama-3.1-405B-Instruct"]
url = "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct"
simple_name = "Llama 3.1 405B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Sorti en juillet 2024, ce modèle est le plus grand modèle de la série Llama de Meta, optimisé pour des tâches complexes de programmation, de mathématique et de raisonnement."
description = "Doté de 405 milliards de paramètres, ce modèle est le plus gros modèle de langage entraîné à ce jour par l'entreprise Meta. Sa taille et son coût élevé en inférence le réservent à des applications exigeantes de raisonnement et de programmation. Le modèle a été entraîné sur un corpus de 15 milliards de tokens dont la limite de connaissances est fixée au mois de décembre 2023."
params = 405
license = "Llama 3.1"
release_date = "07/2024"

["meta-Llama-3.1-8B-Instruct"]
url = "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
simple_name = "Llama 3.1 8B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Version la plus légère de Llama 3.1, ce modèle est rapide et adapté aux applications courantes. Sorti en avril 2024, les données sur lesquelles il a été entraîné remontent au mois de décembre 2023."
description = "Lancé en avril 2024, ce modèle cadet de la famille Llama 3.1 a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d'instructions et d'annotations faites par des humains. Il est adapté aux tâches multilingues et au traitement de longs textes pouvant aller jusqu'à 128 000 tokens."
params = 8
license = "Llama 3.1"
release_date = "04/2024"

["llama-3.1-70b-instruct"]
url = "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct"
params = 70
simple_name = "Llama 3.1 70B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Doté de 70 milliards de paramètres et sorti en avril 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
description = "Comme les autres modèles de la famille Llama 3.1, ce modèle sorti en avril 2024 a été entraîné sur des données qui remontent au mois de décembre 2023. Inutile de l'interroger sur les temps forts des Jeux olympiques de Paris 2024 ! Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
license = "Llama 3.1"
release_date = "04/2024"

["meta-Llama-3.1-70b-instruct-fp8"]
url = "https://huggingface.co/neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8"
simple_name = "Llama 3.1 70B Instruct FP8"
organisation = "Meta / Neuralmagic"
icon_path = "meta.webp"
friendly_size = "XL"
excerpt = "Sorti en avril 2025, ce modèle Llama 3.1 70B Instruct est une version quantifiée FP8 du grand modèle open-source de Meta, optimisée par Neuralmagic pour une inférence plus rapide et une empreinte mémoire réduite."
description = "Sorti en avril 2025, Meta-Llama-3.1-70B-Instruct-FP8 est une version quantifiée FP8 du modèle Llama 3.1 70B Instruct de Meta, proposée par Neuralmagic. Cette quantification permet de réduire la consommation mémoire et d'accélérer l'inférence, tout en conservant des performances proches du modèle d'origine. Il est adapté aux tâches d'assistance conversationnelle et d'instruction-following, avec un contexte étendu, et bénéficie des dernières avancées de Meta sur la série Llama 3.1."
total_params = 70  # En milliards
active_params = 70  # Architecture dense, pas de MoE
license = "Meta Llama 3 Community License"
release_date = "04/2025"

["llama-3.3-70b-instruct"]
url = "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
params = 70
simple_name = "Llama 3.3 70B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Doté de 70 milliards de paramètres et sorti en décembre 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
description = "Remplaçant la famille Llama 3.2, ce modèle sorti en décembre 2024 a été entraîné sur des données qui remontent au mois de décembre 2023. Inutile de l'interroger sur les temps forts des Jeux olympiques de Paris 2024 ! Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
license = "Llama 3.3"
release_date = "12/2024"

["llama-3.1-nemotron-70b-instruct"]
url = "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct"
params = 70
simple_name = "Llama 3.1 Nemotron 70B"
organisation = "NVIDIA"
icon_path = "nvidia.svg"
excerpt = "Doté de 70 milliards de paramètres, ce modèle est un fine-tune conçu par le concepteur de cartes graphiques NVIDIA à partir du modèle Llama 3.1 70B de Meta."
description = "Doté de 70 milliards de paramètres, ce modèle est un fine-tune conçu par le concepteur de cartes graphiques NVIDIA à partir du modèle Llama 3.1 70B de Meta. Sorti en octobre 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
license = "Llama 3.1"
release_date = "10/2024"

["Meta-Llama-3-70B-Instruct"]
params = 70
simple_name = "Llama 3 70B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens mais supporte un contexte relativement restreint de 8000 tokens."
description = "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d'instructions et d'annotations faites par des humains. Il supporte un contexte de 8000 tokens."
license = "Llama 3 Community"
release_date = "04/2024"

["Meta-Llama-3-8B-Instruct"]
params = 8
simple_name = "Llama 3 8B"
organisation = "Meta"
icon_path = "meta.svg"
description = "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l'efficacité et la sécurité."
excerpt = "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l'efficacité et la sécurité."
license = "Llama 3 Community"

["mistral-small-24B-Instruct-2501"]
url = "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
simple_name = "Mistral Small 3"
organisation = "Mistral"
params = 24
icon_path = "mistral.png"
description = "Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme, possède un mode d'appel de fonction et un contexte de 32 000 tokens."
excerpt = "Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme et possède des capacités de raisonnement avancées."
license = "Apache 2.0"
release_date = "01/2025"

["mistral-nemo-2407"]
url = "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
simple_name = "Mistral Nemo"
params = 12
organisation = "Mistral"
icon_path = "mistral.png"
description = "Sorti en juillet 2024, ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes allant jusqu'à 128 000 tokens en plus de 100 langues."
excerpt = "Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter un contexte de 128k tokens en plus de 100 langues. Sorti en juillet 2024."
license = "Apache 2.0"
release_date = "07/2024"

["ministral-8b-instruct-2410"]
url = "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
simple_name = "Ministral"
params = 8
organisation = "Mistral"
icon_path = "mistral.png"
description = "Sorti en octobre 2024, ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes. Il parle plus de 100 langues."
excerpt = "Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter plus de 100 langues. Sorti en octobre 2024."
license = "Mistral AI Non-Production"
release_date = "10/2024"

["Mixtral-8x22B-Instruct-v0.1"]
url = "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1"
simple_name = "Mixtral 8x22B"
active_params = 44
total_params = 176
organisation = "Mistral"
icon_path = "mistral.png"
excerpt = "Ce modèle multilingue sorti en avril 2024 a particulièrement été entraîné en anglais, français, allemand, italien et espagnol ainsi que sur des tâches de mathématiques, programmation et raisonnement."
description = "L'architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents. Sorti en avril 2024."
license = "Apache 2.0"
release_date = "04/2024"

["Mixtral-8x7B-Instruct-v0.1"]
url = "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
simple_name = "Mixtral-8x7B"
organisation = "Mistral"
active_params = 14
total_params = 56
icon_path = "mistral.png"
description = "Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. Grâce à l'architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence."
excerpt = "Ce modèle entraîné sur un corpus multilingue est efficace pour des tâches variées et peu complexes."
license = "Apache 2.0"

["mistral-large-2411"]
url = "https://huggingface.co/mistralai/Mistral-Large-Instruct-2411"
simple_name = "Mistral Large"
organisation = "Mistral"
params = 123
icon_path = "mistral.png"
description = "Sorti en novembre 2024, ce modèle de la société française Mistral gère l'anglais, le français, l'italien, l'allemand et l'espagnol."
excerpt = "Sorti en novembre 2024, ce modèle de la société française Mistral gère l'anglais, le français, l'italien, l'allemand et l'espagnol."
license = "Mistral AI Research"
release_date = "11/2024"

["whisper-large-v3"]
url = "https://huggingface.co/openai/whisper-large-v3"
simple_name = "Whisper Large v3"
organisation = "OpenAI"
icon_path = "openai.svg"
license = "MIT"
params = 1550
excerpt = "Whisper Large v3 est un modèle de reconnaissance automatique de la parole (ASR) multilingue open source, développé par OpenAI. Il est particulièrement performant pour la transcription et la traduction audio."
description = "Whisper Large v3 est la troisième version du modèle ASR open source d'OpenAI. Il prend en charge de nombreuses langues, offre une grande précision sur la transcription et la traduction audio, et est utilisé dans de nombreux outils de transcription automatique. Le modèle est disponible sous licence MIT et hébergé sur Hugging Face."
release_date = "12/2023"
