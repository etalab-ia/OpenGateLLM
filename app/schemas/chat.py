from typing import List, Literal, Optional, Union, Dict, Any

from openai.types.chat import ChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam
from pydantic import BaseModel, Field, model_validator, field_validator

from app.schemas.search import SearchArgs, Search


DEFAULT_RAG_TEMPLATE = "Réponds à la question suivante en te basant sur les documents ci-dessous : {prompt}\n\nDocuments :\n{chunks}"


class ChatSearchArgs(SearchArgs):
    template: str = Field(
        description='Template to use for the RAG query. The template must contain "{chunks}" and "{prompt}" placeholders.',
        default=DEFAULT_RAG_TEMPLATE,
    )

    @field_validator("template")
    def validate_template(cls, value):
        if "{chunks}" not in value:
            raise ValueError('template must contain "{chunks}" placeholder')
        if "{prompt}" not in value:
            raise ValueError('template must contain "{prompt}" placeholder')

        return value


class OpenAIBaseModel(BaseModel):
    class Config:
        extra = "allow"


class JsonSchemaResponseFormat(OpenAIBaseModel):
    name: str = Field(description="The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.")  # fmt: off
    description: Optional[str] = Field(default=None, description="A description of what the response format is for, used by the model to determine how to respond in the format.")  # fmt: off
    json_schema: Optional[Dict[str, Any]] = Field(default=None, alias="schema", description="The schema for the response format, described as a JSON Schema object.")  # fmt: off
    strict: Optional[bool] = Field(default=None, description="Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when `strict` is `true`.")  # fmt: off


class ResponseFormat(OpenAIBaseModel):
    type: Literal["text", "json_object", "json_schema"] = Field(description="The type of the response format.")
    json_schema: Optional[JsonSchemaResponseFormat] = None


class StreamOptions(OpenAIBaseModel):
    include_usage: Optional[bool] = Field(default=True, description="If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.")  # fmt: off
    continuous_usage_stats: Optional[bool] = Field(default=False, description="If set, the `usage` field on each chunk will be updated with the token usage statistics for the entire request. This is useful for monitoring the token usage of a long-running request.")  # fmt: off


class FunctionDefinition(OpenAIBaseModel):
    name: str
    description: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None


class ChatCompletionToolsParam(OpenAIBaseModel):
    type: Literal["function"] = "function"
    function: FunctionDefinition


class ChatCompletionNamedFunction(OpenAIBaseModel):
    name: str


class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel):
    function: ChatCompletionNamedFunction
    type: Literal["function"] = "function"


class ChatCompletionRequest(OpenAIBaseModel):
    # only union between OpenAI fields and vLLM fields are defined. See https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/protocol.py#L209
    messages: List[ChatCompletionMessageParam] = Field(description="A list of messages comprising the conversation so far.")  # fmt: off
    model: str = Field(description="ID of the model to use. Call `/v1/models` endpoint to get the list of available models, only `text-generation` model type is supported.")  # fmt: off
    frequency_penalty: Optional[float] = Field(default=0.0, description="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.")  # fmt: off
    logit_bias: Optional[Dict[str, float]] = Field(default=None, description="Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.")  # fmt: off
    logprobs: Optional[bool] = Field(default=False, description="Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.")  # fmt: off
    top_logprobs: Optional[int] = Field(default=None, description="An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.")  # fmt: off
    presence_penalty: Optional[float] = Field(default=0.0, description="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.")  # fmt: off
    max_tokens: Optional[int] = Field(default=None, description="The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.", deprecated=True)  # fmt: off
    max_completion_tokens: Optional[int] = Field(default=None, description="An upper bound for the number of tokens that can be generated for a completion.")  # fmt: off
    n: Optional[int] = Field(default=1, description="How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.")  # fmt: off
    presence_penalty: Optional[float] = Field(default=0.0, description="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.")  # fmt: off
    response_format: Optional[ResponseFormat] = Field(default=None, description="Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs guide. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.<br>**Important**: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if `finish_reason=\"length\"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.")  # fmt: off
    seed: Optional[int] = Field(default=None, description="If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.")  # fmt: off
    stop: Optional[Union[str, List[str]]] = Field(default_factory=list, description="Up to 4 sequences where the API will stop generating further tokens.")  # fmt: off
    stream: Optional[Literal[True, False]] = Field(default=False, description="If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.")  # fmt: off
    stream_options: Optional[StreamOptions] = Field(default=None, description="Options for streaming response. Only set this when you set `stream: true`.")  # fmt: off
    temperature: Optional[float] = Field(default=0.7, description="What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.")  # fmt: off
    top_p: Optional[float] = Field(default=1, description="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.<br>We generally recommend altering this or `temperature` but not both.")  # fmt: off

    # @TODO: check if tool params works
    # tools: Optional[List[ChatCompletionToolsParam]] = Field(default=None, description="A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.")  # fmt: off
    # tool_choice: Optional[Union[Literal["none"], Literal["auto"], ChatCompletionNamedToolChoiceParam]] = Field(default="none", description="Controls which (if any) tool is called by the model. `none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools. Specifying a particular tool via `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool.<br>`none` is the default when no tools are present. `auto` is the default if tools are present.")  # fmt: off

    # @TODO: add support multimodal models

    # search additionnal fields
    search: bool = Field(default=False, deprecated=True)  # fmt: off
    search_args: Optional[ChatSearchArgs] = Field(default=None, deprecated=True)  # fmt: off

    @model_validator(mode="after")
    def validate_model(cls, values):
        if values.search:
            if not values.search_args:
                raise ValueError("search_args is required when search is true")

        return values


class ChatCompletion(ChatCompletion):
    search_results: List[Search] = []


class ChatCompletionChunk(ChatCompletionChunk):
    search_results: List[Search] = []
